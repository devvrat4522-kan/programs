{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "#### y = $a_{0} + a_{1}x + ε$\n",
    "y = Dependent Variable (Target Variable)<br>x = Independent Variable (predictor Variable)<br>\n",
    "$a_{0}$ = intercept of the line (Gives an additional degree of freedom)<br>\n",
    "$a_{1}$ = Linear regression coefficient (scale factor to each input value).<br>ε = random error\n",
    "<br>\n",
    "### Cost function\n",
    "When working with linear regression, our main goal is to find the best fit line that means the error between predicted values and actual values should be minimized. The best fit line will have the least error.The different values for weights or coefficient of lines (a0, a1) gives the different line of regression, and the cost function is used to estimate the values of the coefficient for the best fit line.\n",
    "Cost function optimizes the regression coefficients or weights. It measures how a linear regression model is performing.<br>For Linear Regression, we use the Mean Squared Error (MSE) cost function, which is the average of squared error occurred between the predicted values and actual values. It can be written as:<br>\n",
    "$MSE = \\frac{1}{N} \\sum \\limits_{i=1}^{n}(y_{i} - (a_{1}x_{i} + a_{0}))^2$<br>\n",
    "where, N = Total number of observations<br>\n",
    "$y_{i}$ = Actual value<br>\n",
    "$(a_{1}x_{i} + a_{0})$ = Predicted Value\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "Algorithm that models the relationship between a dependent variable and a single independent variable. The relationship shown by a Simple Linear Regression model is linear or a sloped straight line, hence it is called Simple Linear Regression.<br>\n",
    "The key point in Simple Linear Regression is that the <b><i>dependent variable must be a continuous/real value</i></b>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Problem Statement example for Simple Linear Regression:</b><br>\n",
    "Here we are taking a dataset that has two variables: <b>salary (dependent variable)</b> and <b>experience (Independent variable)</b>.<br>The goals of this problem is:\n",
    "<ul>\n",
    "<li>We want to find out if there is any correlation between these two variables</li>\n",
    "<li>We will find the best fit line for the dataset.</li>\n",
    "<li>How the dependent variable is changing by changing the independent variable.</li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for managing data\n",
    "import matplotlib.pyplot as plt # for plotting the graph\n",
    "import pandas as pd # for reading data\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset into training and test set\n",
    "from sklearn.linear_model import LinearRegression #for fitting the Simple Linear Regression model to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearsExperience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>39343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3</td>\n",
       "      <td>46205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>37731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>43525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2</td>\n",
       "      <td>39891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.9</td>\n",
       "      <td>56642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>60150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.2</td>\n",
       "      <td>54445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.2</td>\n",
       "      <td>64445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.7</td>\n",
       "      <td>57189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.9</td>\n",
       "      <td>63218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.0</td>\n",
       "      <td>55794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.0</td>\n",
       "      <td>56957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.1</td>\n",
       "      <td>57081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.5</td>\n",
       "      <td>61111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.9</td>\n",
       "      <td>67938.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.1</td>\n",
       "      <td>66029.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.3</td>\n",
       "      <td>83088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.9</td>\n",
       "      <td>81363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.0</td>\n",
       "      <td>93940.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.8</td>\n",
       "      <td>91738.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.1</td>\n",
       "      <td>98273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.9</td>\n",
       "      <td>101302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.2</td>\n",
       "      <td>113812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8.7</td>\n",
       "      <td>109431.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.0</td>\n",
       "      <td>105582.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.5</td>\n",
       "      <td>116969.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.6</td>\n",
       "      <td>112635.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.3</td>\n",
       "      <td>122391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.5</td>\n",
       "      <td>121872.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YearsExperience    Salary\n",
       "0               1.1   39343.0\n",
       "1               1.3   46205.0\n",
       "2               1.5   37731.0\n",
       "3               2.0   43525.0\n",
       "4               2.2   39891.0\n",
       "5               2.9   56642.0\n",
       "6               3.0   60150.0\n",
       "7               3.2   54445.0\n",
       "8               3.2   64445.0\n",
       "9               3.7   57189.0\n",
       "10              3.9   63218.0\n",
       "11              4.0   55794.0\n",
       "12              4.0   56957.0\n",
       "13              4.1   57081.0\n",
       "14              4.5   61111.0\n",
       "15              4.9   67938.0\n",
       "16              5.1   66029.0\n",
       "17              5.3   83088.0\n",
       "18              5.9   81363.0\n",
       "19              6.0   93940.0\n",
       "20              6.8   91738.0\n",
       "21              7.1   98273.0\n",
       "22              7.9  101302.0\n",
       "23              8.2  113812.0\n",
       "24              8.7  109431.0\n",
       "25              9.0  105582.0\n",
       "26              9.5  116969.0\n",
       "27              9.6  112635.0\n",
       "28             10.3  122391.0\n",
       "29             10.5  121872.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading dataset into our code\n",
    "data = pd.read_csv('Salary_Data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " [[ 1.1]\n",
      " [ 1.3]\n",
      " [ 1.5]\n",
      " [ 2. ]\n",
      " [ 2.2]\n",
      " [ 2.9]\n",
      " [ 3. ]\n",
      " [ 3.2]\n",
      " [ 3.2]\n",
      " [ 3.7]\n",
      " [ 3.9]\n",
      " [ 4. ]\n",
      " [ 4. ]\n",
      " [ 4.1]\n",
      " [ 4.5]\n",
      " [ 4.9]\n",
      " [ 5.1]\n",
      " [ 5.3]\n",
      " [ 5.9]\n",
      " [ 6. ]\n",
      " [ 6.8]\n",
      " [ 7.1]\n",
      " [ 7.9]\n",
      " [ 8.2]\n",
      " [ 8.7]\n",
      " [ 9. ]\n",
      " [ 9.5]\n",
      " [ 9.6]\n",
      " [10.3]\n",
      " [10.5]] \n",
      "y: \n",
      " [ 39343.  46205.  37731.  43525.  39891.  56642.  60150.  54445.  64445.\n",
      "  57189.  63218.  55794.  56957.  57081.  61111.  67938.  66029.  83088.\n",
      "  81363.  93940.  91738.  98273. 101302. 113812. 109431. 105582. 116969.\n",
      " 112635. 122391. 121872.]\n"
     ]
    }
   ],
   "source": [
    "# we need to extract the dependent(salary) and independent(years of experience) variables from the given dataset\n",
    "x = data.iloc[:, :-1].values ## iloc[] is used to extract the required rows and columns from the dataset\n",
    "y = data.iloc[:, 1].values\n",
    "print(\"x: \\n\",x,\"\\ny: \\n\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will split both variables into the test set and training set\n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 1/3, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 2: Fitting the Simple Linear Regression to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have fitted our regressor object to the training set so that the model can easily learn \n",
    "# the correlations between the predictor and target variables.\n",
    "regressor= LinearRegression()\n",
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 3: Prediction of test result</b>\n",
    "In this step, we will provide the test dataset (new observations) to the model to check whether it can predict the correct output or not.<br>\n",
    "We will create a prediction vector y_pred, and x_pred, which will contain predictions of test dataset, and prediction of training set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "x_pred = regressor.predict(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 4: Visualizing the Training set results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_train, y_train, color=\"orange\")\n",
    "plt.plot(x_train, x_pred, color=\"magenta\")\n",
    "plt.title(\"Salary vs Experience (Training Dataset)\")\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary(In Rupees)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, we can see the real values observations in orange dots and predicted values are covered by the magenta regression line. The regression line shows a correlation between the dependent and independent variable.\n",
    "The good fit of the line can be observed by calculating the difference between actual values and predicted values. But as we can see in the above plot, most of the observations are close to the regression line, hence our model is good for the training set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 5: Visualizing the Test set results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_test, y_test, color=\"blue\")\n",
    "plt.plot(x_train, x_pred, color=\"magenta\")\n",
    "plt.title(\"Salary vs Experience (Test Dataset)\")\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary(In Rupees)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, there are observations given by the blue color, and prediction is given by the magenta regression line. As we can see, most of the observations are close to the regression line, hence we can say our Simple Linear Regression is a good model and able to make good predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "This algorithm is one of the important regression algorithms which models the linear relationship between a single dependent continuous variable and more than one independent variable.\n",
    "<ul>\n",
    "<li>For MLR, the dependent or target variable(Y) must be the continuous/real, but the predictor or independent variable may be of continuous or categorical form.</li>\n",
    "<li>Each feature variable must model the linear relationship with the dependent variable.</li>\n",
    "<li>MLR tries to fit a regression line through a multidimensional space of data-points.</li>\n",
    "</ul>\n",
    "\n",
    "$y = b_{0}+b_{1}x_{1}+ b_{2}x_{2}+ b_{3}x_{3}+...... b_{n}x_{n}$\n",
    "<br>Where,\n",
    "\n",
    "$y$ = Output/Response variable<br>\n",
    "$b_{0}, b_{1}, b_{2}, b_{3},...., b_{n}$ = Coefficients of the model.<br>\n",
    "$x_{1}, x_{2}, x_{3}, x_{4},...$ = Various Independent/feature variable<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Problem Description:</b><br>\n",
    "We have a dataset of 50 start-up companies. This dataset contains five main information: R&D Spend, Administration Spend, Marketing Spend, State, and Profit for a financial year. Our goal is to create a model that can easily determine which company has a maximum profit, and which is the most affecting factor for the profit of a company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for managing data\n",
    "import matplotlib.pyplot as plt # for plotting the graph\n",
    "import pandas as pd # for reading data\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset into training and test set\n",
    "from sklearn.linear_model import LinearRegression #for fitting the Simple Linear Regression model to the training dataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>131876.90</td>\n",
       "      <td>99814.71</td>\n",
       "      <td>362861.36</td>\n",
       "      <td>New York</td>\n",
       "      <td>156991.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>134615.46</td>\n",
       "      <td>147198.87</td>\n",
       "      <td>127716.82</td>\n",
       "      <td>California</td>\n",
       "      <td>156122.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130298.13</td>\n",
       "      <td>145530.06</td>\n",
       "      <td>323876.68</td>\n",
       "      <td>Florida</td>\n",
       "      <td>155752.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>120542.52</td>\n",
       "      <td>148718.95</td>\n",
       "      <td>311613.29</td>\n",
       "      <td>New York</td>\n",
       "      <td>152211.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>123334.88</td>\n",
       "      <td>108679.17</td>\n",
       "      <td>304981.62</td>\n",
       "      <td>California</td>\n",
       "      <td>149759.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>101913.08</td>\n",
       "      <td>110594.11</td>\n",
       "      <td>229160.95</td>\n",
       "      <td>Florida</td>\n",
       "      <td>146121.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100671.96</td>\n",
       "      <td>91790.61</td>\n",
       "      <td>249744.55</td>\n",
       "      <td>California</td>\n",
       "      <td>144259.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>93863.75</td>\n",
       "      <td>127320.38</td>\n",
       "      <td>249839.44</td>\n",
       "      <td>Florida</td>\n",
       "      <td>141585.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>91992.39</td>\n",
       "      <td>135495.07</td>\n",
       "      <td>252664.93</td>\n",
       "      <td>California</td>\n",
       "      <td>134307.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>119943.24</td>\n",
       "      <td>156547.42</td>\n",
       "      <td>256512.92</td>\n",
       "      <td>Florida</td>\n",
       "      <td>132602.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>114523.61</td>\n",
       "      <td>122616.84</td>\n",
       "      <td>261776.23</td>\n",
       "      <td>New York</td>\n",
       "      <td>129917.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78013.11</td>\n",
       "      <td>121597.55</td>\n",
       "      <td>264346.06</td>\n",
       "      <td>California</td>\n",
       "      <td>126992.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>94657.16</td>\n",
       "      <td>145077.58</td>\n",
       "      <td>282574.31</td>\n",
       "      <td>New York</td>\n",
       "      <td>125370.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>91749.16</td>\n",
       "      <td>114175.79</td>\n",
       "      <td>294919.57</td>\n",
       "      <td>Florida</td>\n",
       "      <td>124266.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>86419.70</td>\n",
       "      <td>153514.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>New York</td>\n",
       "      <td>122776.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>76253.86</td>\n",
       "      <td>113867.30</td>\n",
       "      <td>298664.47</td>\n",
       "      <td>California</td>\n",
       "      <td>118474.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>78389.47</td>\n",
       "      <td>153773.43</td>\n",
       "      <td>299737.29</td>\n",
       "      <td>New York</td>\n",
       "      <td>111313.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>73994.56</td>\n",
       "      <td>122782.75</td>\n",
       "      <td>303319.26</td>\n",
       "      <td>Florida</td>\n",
       "      <td>110352.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>67532.53</td>\n",
       "      <td>105751.03</td>\n",
       "      <td>304768.73</td>\n",
       "      <td>Florida</td>\n",
       "      <td>108733.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>77044.01</td>\n",
       "      <td>99281.34</td>\n",
       "      <td>140574.81</td>\n",
       "      <td>New York</td>\n",
       "      <td>108552.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64664.71</td>\n",
       "      <td>139553.16</td>\n",
       "      <td>137962.62</td>\n",
       "      <td>California</td>\n",
       "      <td>107404.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>75328.87</td>\n",
       "      <td>144135.98</td>\n",
       "      <td>134050.07</td>\n",
       "      <td>Florida</td>\n",
       "      <td>105733.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>72107.60</td>\n",
       "      <td>127864.55</td>\n",
       "      <td>353183.81</td>\n",
       "      <td>New York</td>\n",
       "      <td>105008.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>66051.52</td>\n",
       "      <td>182645.56</td>\n",
       "      <td>118148.20</td>\n",
       "      <td>Florida</td>\n",
       "      <td>103282.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>65605.48</td>\n",
       "      <td>153032.06</td>\n",
       "      <td>107138.38</td>\n",
       "      <td>New York</td>\n",
       "      <td>101004.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>61994.48</td>\n",
       "      <td>115641.28</td>\n",
       "      <td>91131.24</td>\n",
       "      <td>Florida</td>\n",
       "      <td>99937.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>61136.38</td>\n",
       "      <td>152701.92</td>\n",
       "      <td>88218.23</td>\n",
       "      <td>New York</td>\n",
       "      <td>97483.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>63408.86</td>\n",
       "      <td>129219.61</td>\n",
       "      <td>46085.25</td>\n",
       "      <td>California</td>\n",
       "      <td>97427.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>55493.95</td>\n",
       "      <td>103057.49</td>\n",
       "      <td>214634.81</td>\n",
       "      <td>Florida</td>\n",
       "      <td>96778.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>46426.07</td>\n",
       "      <td>157693.92</td>\n",
       "      <td>210797.67</td>\n",
       "      <td>California</td>\n",
       "      <td>96712.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>46014.02</td>\n",
       "      <td>85047.44</td>\n",
       "      <td>205517.64</td>\n",
       "      <td>New York</td>\n",
       "      <td>96479.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>28663.76</td>\n",
       "      <td>127056.21</td>\n",
       "      <td>201126.82</td>\n",
       "      <td>Florida</td>\n",
       "      <td>90708.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>44069.95</td>\n",
       "      <td>51283.14</td>\n",
       "      <td>197029.42</td>\n",
       "      <td>California</td>\n",
       "      <td>89949.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20229.59</td>\n",
       "      <td>65947.93</td>\n",
       "      <td>185265.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>81229.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>38558.51</td>\n",
       "      <td>82982.09</td>\n",
       "      <td>174999.30</td>\n",
       "      <td>California</td>\n",
       "      <td>81005.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>28754.33</td>\n",
       "      <td>118546.05</td>\n",
       "      <td>172795.67</td>\n",
       "      <td>California</td>\n",
       "      <td>78239.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>27892.92</td>\n",
       "      <td>84710.77</td>\n",
       "      <td>164470.71</td>\n",
       "      <td>Florida</td>\n",
       "      <td>77798.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>23640.93</td>\n",
       "      <td>96189.63</td>\n",
       "      <td>148001.11</td>\n",
       "      <td>California</td>\n",
       "      <td>71498.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>15505.73</td>\n",
       "      <td>127382.30</td>\n",
       "      <td>35534.17</td>\n",
       "      <td>New York</td>\n",
       "      <td>69758.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>22177.74</td>\n",
       "      <td>154806.14</td>\n",
       "      <td>28334.72</td>\n",
       "      <td>California</td>\n",
       "      <td>65200.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1000.23</td>\n",
       "      <td>124153.04</td>\n",
       "      <td>1903.93</td>\n",
       "      <td>New York</td>\n",
       "      <td>64926.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1315.46</td>\n",
       "      <td>115816.21</td>\n",
       "      <td>297114.46</td>\n",
       "      <td>Florida</td>\n",
       "      <td>49490.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>135426.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>California</td>\n",
       "      <td>42559.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>542.05</td>\n",
       "      <td>51743.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>New York</td>\n",
       "      <td>35673.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.00</td>\n",
       "      <td>116983.80</td>\n",
       "      <td>45173.06</td>\n",
       "      <td>California</td>\n",
       "      <td>14681.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0   165349.20       136897.80        471784.10    New York  192261.83\n",
       "1   162597.70       151377.59        443898.53  California  191792.06\n",
       "2   153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3   144372.41       118671.85        383199.62    New York  182901.99\n",
       "4   142107.34        91391.77        366168.42     Florida  166187.94\n",
       "5   131876.90        99814.71        362861.36    New York  156991.12\n",
       "6   134615.46       147198.87        127716.82  California  156122.51\n",
       "7   130298.13       145530.06        323876.68     Florida  155752.60\n",
       "8   120542.52       148718.95        311613.29    New York  152211.77\n",
       "9   123334.88       108679.17        304981.62  California  149759.96\n",
       "10  101913.08       110594.11        229160.95     Florida  146121.95\n",
       "11  100671.96        91790.61        249744.55  California  144259.40\n",
       "12   93863.75       127320.38        249839.44     Florida  141585.52\n",
       "13   91992.39       135495.07        252664.93  California  134307.35\n",
       "14  119943.24       156547.42        256512.92     Florida  132602.65\n",
       "15  114523.61       122616.84        261776.23    New York  129917.04\n",
       "16   78013.11       121597.55        264346.06  California  126992.93\n",
       "17   94657.16       145077.58        282574.31    New York  125370.37\n",
       "18   91749.16       114175.79        294919.57     Florida  124266.90\n",
       "19   86419.70       153514.11             0.00    New York  122776.86\n",
       "20   76253.86       113867.30        298664.47  California  118474.03\n",
       "21   78389.47       153773.43        299737.29    New York  111313.02\n",
       "22   73994.56       122782.75        303319.26     Florida  110352.25\n",
       "23   67532.53       105751.03        304768.73     Florida  108733.99\n",
       "24   77044.01        99281.34        140574.81    New York  108552.04\n",
       "25   64664.71       139553.16        137962.62  California  107404.34\n",
       "26   75328.87       144135.98        134050.07     Florida  105733.54\n",
       "27   72107.60       127864.55        353183.81    New York  105008.31\n",
       "28   66051.52       182645.56        118148.20     Florida  103282.38\n",
       "29   65605.48       153032.06        107138.38    New York  101004.64\n",
       "30   61994.48       115641.28         91131.24     Florida   99937.59\n",
       "31   61136.38       152701.92         88218.23    New York   97483.56\n",
       "32   63408.86       129219.61         46085.25  California   97427.84\n",
       "33   55493.95       103057.49        214634.81     Florida   96778.92\n",
       "34   46426.07       157693.92        210797.67  California   96712.80\n",
       "35   46014.02        85047.44        205517.64    New York   96479.51\n",
       "36   28663.76       127056.21        201126.82     Florida   90708.19\n",
       "37   44069.95        51283.14        197029.42  California   89949.14\n",
       "38   20229.59        65947.93        185265.10    New York   81229.06\n",
       "39   38558.51        82982.09        174999.30  California   81005.76\n",
       "40   28754.33       118546.05        172795.67  California   78239.91\n",
       "41   27892.92        84710.77        164470.71     Florida   77798.83\n",
       "42   23640.93        96189.63        148001.11  California   71498.49\n",
       "43   15505.73       127382.30         35534.17    New York   69758.98\n",
       "44   22177.74       154806.14         28334.72  California   65200.33\n",
       "45    1000.23       124153.04          1903.93    New York   64926.08\n",
       "46    1315.46       115816.21        297114.46     Florida   49490.75\n",
       "47       0.00       135426.92             0.00  California   42559.73\n",
       "48     542.05        51743.15             0.00    New York   35673.41\n",
       "49       0.00       116983.80         45173.06  California   14681.40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading dataset into our code\n",
    "data = pd.read_csv('50_Startups.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " [[165349.2 136897.8 471784.1 'New York']\n",
      " [162597.7 151377.59 443898.53 'California']\n",
      " [153441.51 101145.55 407934.54 'Florida']\n",
      " [144372.41 118671.85 383199.62 'New York']\n",
      " [142107.34 91391.77 366168.42 'Florida']\n",
      " [131876.9 99814.71 362861.36 'New York']\n",
      " [134615.46 147198.87 127716.82 'California']\n",
      " [130298.13 145530.06 323876.68 'Florida']\n",
      " [120542.52 148718.95 311613.29 'New York']\n",
      " [123334.88 108679.17 304981.62 'California']\n",
      " [101913.08 110594.11 229160.95 'Florida']\n",
      " [100671.96 91790.61 249744.55 'California']\n",
      " [93863.75 127320.38 249839.44 'Florida']\n",
      " [91992.39 135495.07 252664.93 'California']\n",
      " [119943.24 156547.42 256512.92 'Florida']\n",
      " [114523.61 122616.84 261776.23 'New York']\n",
      " [78013.11 121597.55 264346.06 'California']\n",
      " [94657.16 145077.58 282574.31 'New York']\n",
      " [91749.16 114175.79 294919.57 'Florida']\n",
      " [86419.7 153514.11 0.0 'New York']\n",
      " [76253.86 113867.3 298664.47 'California']\n",
      " [78389.47 153773.43 299737.29 'New York']\n",
      " [73994.56 122782.75 303319.26 'Florida']\n",
      " [67532.53 105751.03 304768.73 'Florida']\n",
      " [77044.01 99281.34 140574.81 'New York']\n",
      " [64664.71 139553.16 137962.62 'California']\n",
      " [75328.87 144135.98 134050.07 'Florida']\n",
      " [72107.6 127864.55 353183.81 'New York']\n",
      " [66051.52 182645.56 118148.2 'Florida']\n",
      " [65605.48 153032.06 107138.38 'New York']\n",
      " [61994.48 115641.28 91131.24 'Florida']\n",
      " [61136.38 152701.92 88218.23 'New York']\n",
      " [63408.86 129219.61 46085.25 'California']\n",
      " [55493.95 103057.49 214634.81 'Florida']\n",
      " [46426.07 157693.92 210797.67 'California']\n",
      " [46014.02 85047.44 205517.64 'New York']\n",
      " [28663.76 127056.21 201126.82 'Florida']\n",
      " [44069.95 51283.14 197029.42 'California']\n",
      " [20229.59 65947.93 185265.1 'New York']\n",
      " [38558.51 82982.09 174999.3 'California']\n",
      " [28754.33 118546.05 172795.67 'California']\n",
      " [27892.92 84710.77 164470.71 'Florida']\n",
      " [23640.93 96189.63 148001.11 'California']\n",
      " [15505.73 127382.3 35534.17 'New York']\n",
      " [22177.74 154806.14 28334.72 'California']\n",
      " [1000.23 124153.04 1903.93 'New York']\n",
      " [1315.46 115816.21 297114.46 'Florida']\n",
      " [0.0 135426.92 0.0 'California']\n",
      " [542.05 51743.15 0.0 'New York']\n",
      " [0.0 116983.8 45173.06 'California']] \n",
      "y: \n",
      " [192261.83 191792.06 191050.39 182901.99 166187.94 156991.12 156122.51\n",
      " 155752.6  152211.77 149759.96 146121.95 144259.4  141585.52 134307.35\n",
      " 132602.65 129917.04 126992.93 125370.37 124266.9  122776.86 118474.03\n",
      " 111313.02 110352.25 108733.99 108552.04 107404.34 105733.54 105008.31\n",
      " 103282.38 101004.64  99937.59  97483.56  97427.84  96778.92  96712.8\n",
      "  96479.51  90708.19  89949.14  81229.06  81005.76  78239.91  77798.83\n",
      "  71498.49  69758.98  65200.33  64926.08  49490.75  42559.73  35673.41\n",
      "  14681.4 ]\n"
     ]
    }
   ],
   "source": [
    "# we need to extract the dependent(profit) and independent(R&D spend, Marketing Spend, Administration and State) variables from the given dataset\n",
    "x = data.iloc[:, :-1].values ## iloc[] is used to extract the required rows and columns from the dataset\n",
    "y = data.iloc[:, 4].values\n",
    "print(\"x: \\n\",x,\"\\ny: \\n\",y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above output, the last column contains categorical variables which are not suitable to apply directly for fitting the model. So we need to encode this variable.To encode the categorical variable into numbers, we will use the LabelEncoder class. But it is not sufficient because it still has some relational order, which may create a wrong model. So in order to remove this problem, we will use OneHotEncoder, which will create the dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 1.0, 165349.2, 136897.8, 471784.1],\n",
       "       [1.0, 0.0, 0.0, 162597.7, 151377.59, 443898.53],\n",
       "       [0.0, 1.0, 0.0, 153441.51, 101145.55, 407934.54],\n",
       "       [0.0, 0.0, 1.0, 144372.41, 118671.85, 383199.62],\n",
       "       [0.0, 1.0, 0.0, 142107.34, 91391.77, 366168.42],\n",
       "       [0.0, 0.0, 1.0, 131876.9, 99814.71, 362861.36],\n",
       "       [1.0, 0.0, 0.0, 134615.46, 147198.87, 127716.82],\n",
       "       [0.0, 1.0, 0.0, 130298.13, 145530.06, 323876.68],\n",
       "       [0.0, 0.0, 1.0, 120542.52, 148718.95, 311613.29],\n",
       "       [1.0, 0.0, 0.0, 123334.88, 108679.17, 304981.62],\n",
       "       [0.0, 1.0, 0.0, 101913.08, 110594.11, 229160.95],\n",
       "       [1.0, 0.0, 0.0, 100671.96, 91790.61, 249744.55],\n",
       "       [0.0, 1.0, 0.0, 93863.75, 127320.38, 249839.44],\n",
       "       [1.0, 0.0, 0.0, 91992.39, 135495.07, 252664.93],\n",
       "       [0.0, 1.0, 0.0, 119943.24, 156547.42, 256512.92],\n",
       "       [0.0, 0.0, 1.0, 114523.61, 122616.84, 261776.23],\n",
       "       [1.0, 0.0, 0.0, 78013.11, 121597.55, 264346.06],\n",
       "       [0.0, 0.0, 1.0, 94657.16, 145077.58, 282574.31],\n",
       "       [0.0, 1.0, 0.0, 91749.16, 114175.79, 294919.57],\n",
       "       [0.0, 0.0, 1.0, 86419.7, 153514.11, 0.0],\n",
       "       [1.0, 0.0, 0.0, 76253.86, 113867.3, 298664.47],\n",
       "       [0.0, 0.0, 1.0, 78389.47, 153773.43, 299737.29],\n",
       "       [0.0, 1.0, 0.0, 73994.56, 122782.75, 303319.26],\n",
       "       [0.0, 1.0, 0.0, 67532.53, 105751.03, 304768.73],\n",
       "       [0.0, 0.0, 1.0, 77044.01, 99281.34, 140574.81],\n",
       "       [1.0, 0.0, 0.0, 64664.71, 139553.16, 137962.62],\n",
       "       [0.0, 1.0, 0.0, 75328.87, 144135.98, 134050.07],\n",
       "       [0.0, 0.0, 1.0, 72107.6, 127864.55, 353183.81],\n",
       "       [0.0, 1.0, 0.0, 66051.52, 182645.56, 118148.2],\n",
       "       [0.0, 0.0, 1.0, 65605.48, 153032.06, 107138.38],\n",
       "       [0.0, 1.0, 0.0, 61994.48, 115641.28, 91131.24],\n",
       "       [0.0, 0.0, 1.0, 61136.38, 152701.92, 88218.23],\n",
       "       [1.0, 0.0, 0.0, 63408.86, 129219.61, 46085.25],\n",
       "       [0.0, 1.0, 0.0, 55493.95, 103057.49, 214634.81],\n",
       "       [1.0, 0.0, 0.0, 46426.07, 157693.92, 210797.67],\n",
       "       [0.0, 0.0, 1.0, 46014.02, 85047.44, 205517.64],\n",
       "       [0.0, 1.0, 0.0, 28663.76, 127056.21, 201126.82],\n",
       "       [1.0, 0.0, 0.0, 44069.95, 51283.14, 197029.42],\n",
       "       [0.0, 0.0, 1.0, 20229.59, 65947.93, 185265.1],\n",
       "       [1.0, 0.0, 0.0, 38558.51, 82982.09, 174999.3],\n",
       "       [1.0, 0.0, 0.0, 28754.33, 118546.05, 172795.67],\n",
       "       [0.0, 1.0, 0.0, 27892.92, 84710.77, 164470.71],\n",
       "       [1.0, 0.0, 0.0, 23640.93, 96189.63, 148001.11],\n",
       "       [0.0, 0.0, 1.0, 15505.73, 127382.3, 35534.17],\n",
       "       [1.0, 0.0, 0.0, 22177.74, 154806.14, 28334.72],\n",
       "       [0.0, 0.0, 1.0, 1000.23, 124153.04, 1903.93],\n",
       "       [0.0, 1.0, 0.0, 1315.46, 115816.21, 297114.46],\n",
       "       [1.0, 0.0, 0.0, 0.0, 135426.92, 0.0],\n",
       "       [0.0, 0.0, 1.0, 542.05, 51743.15, 0.0],\n",
       "       [1.0, 0.0, 0.0, 0.0, 116983.8, 45173.06]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Catgorical data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "labelencoder_x= LabelEncoder()\n",
    "x[:, 3]= labelencoder_x.fit_transform(x[:,3])\n",
    "ct = ColumnTransformer([(\"State\", OneHotEncoder(), [3])], remainder = 'passthrough')\n",
    "x = ct.fit_transform(x)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>*Note:</b> We should not use all the dummy variables at the same time, so it must be 1 less than the total number of dummy variables, else it will create a dummy variable trap. If we do not remove the first dummy variable, then it may introduce multicollinearity in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 165349.2, 136897.8, 471784.1],\n",
       "       [0.0, 0.0, 162597.7, 151377.59, 443898.53],\n",
       "       [1.0, 0.0, 153441.51, 101145.55, 407934.54],\n",
       "       [0.0, 1.0, 144372.41, 118671.85, 383199.62],\n",
       "       [1.0, 0.0, 142107.34, 91391.77, 366168.42],\n",
       "       [0.0, 1.0, 131876.9, 99814.71, 362861.36],\n",
       "       [0.0, 0.0, 134615.46, 147198.87, 127716.82],\n",
       "       [1.0, 0.0, 130298.13, 145530.06, 323876.68],\n",
       "       [0.0, 1.0, 120542.52, 148718.95, 311613.29],\n",
       "       [0.0, 0.0, 123334.88, 108679.17, 304981.62],\n",
       "       [1.0, 0.0, 101913.08, 110594.11, 229160.95],\n",
       "       [0.0, 0.0, 100671.96, 91790.61, 249744.55],\n",
       "       [1.0, 0.0, 93863.75, 127320.38, 249839.44],\n",
       "       [0.0, 0.0, 91992.39, 135495.07, 252664.93],\n",
       "       [1.0, 0.0, 119943.24, 156547.42, 256512.92],\n",
       "       [0.0, 1.0, 114523.61, 122616.84, 261776.23],\n",
       "       [0.0, 0.0, 78013.11, 121597.55, 264346.06],\n",
       "       [0.0, 1.0, 94657.16, 145077.58, 282574.31],\n",
       "       [1.0, 0.0, 91749.16, 114175.79, 294919.57],\n",
       "       [0.0, 1.0, 86419.7, 153514.11, 0.0],\n",
       "       [0.0, 0.0, 76253.86, 113867.3, 298664.47],\n",
       "       [0.0, 1.0, 78389.47, 153773.43, 299737.29],\n",
       "       [1.0, 0.0, 73994.56, 122782.75, 303319.26],\n",
       "       [1.0, 0.0, 67532.53, 105751.03, 304768.73],\n",
       "       [0.0, 1.0, 77044.01, 99281.34, 140574.81],\n",
       "       [0.0, 0.0, 64664.71, 139553.16, 137962.62],\n",
       "       [1.0, 0.0, 75328.87, 144135.98, 134050.07],\n",
       "       [0.0, 1.0, 72107.6, 127864.55, 353183.81],\n",
       "       [1.0, 0.0, 66051.52, 182645.56, 118148.2],\n",
       "       [0.0, 1.0, 65605.48, 153032.06, 107138.38],\n",
       "       [1.0, 0.0, 61994.48, 115641.28, 91131.24],\n",
       "       [0.0, 1.0, 61136.38, 152701.92, 88218.23],\n",
       "       [0.0, 0.0, 63408.86, 129219.61, 46085.25],\n",
       "       [1.0, 0.0, 55493.95, 103057.49, 214634.81],\n",
       "       [0.0, 0.0, 46426.07, 157693.92, 210797.67],\n",
       "       [0.0, 1.0, 46014.02, 85047.44, 205517.64],\n",
       "       [1.0, 0.0, 28663.76, 127056.21, 201126.82],\n",
       "       [0.0, 0.0, 44069.95, 51283.14, 197029.42],\n",
       "       [0.0, 1.0, 20229.59, 65947.93, 185265.1],\n",
       "       [0.0, 0.0, 38558.51, 82982.09, 174999.3],\n",
       "       [0.0, 0.0, 28754.33, 118546.05, 172795.67],\n",
       "       [1.0, 0.0, 27892.92, 84710.77, 164470.71],\n",
       "       [0.0, 0.0, 23640.93, 96189.63, 148001.11],\n",
       "       [0.0, 1.0, 15505.73, 127382.3, 35534.17],\n",
       "       [0.0, 0.0, 22177.74, 154806.14, 28334.72],\n",
       "       [0.0, 1.0, 1000.23, 124153.04, 1903.93],\n",
       "       [1.0, 0.0, 1315.46, 115816.21, 297114.46],\n",
       "       [0.0, 0.0, 0.0, 135426.92, 0.0],\n",
       "       [0.0, 1.0, 542.05, 51743.15, 0.0],\n",
       "       [0.0, 0.0, 0.0, 116983.8, 45173.06]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#avoiding the dummy variable trap:  \n",
    "x = x[:, 1:]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will split both variables into the test set and training set\n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 2: Fitting the Simple Linear Regression to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have fitted our regressor object to the training set so that the model can easily learn \n",
    "# the correlations between the predictor and target variables.\n",
    "regressor= LinearRegression()\n",
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 3: Prediction of test result</b>\n",
    "In this step, we will provide the test dataset (new observations) to the model to check whether it can predict the correct output or not.<br>\n",
    "We will create a prediction vector y_pred, and x_pred, which will contain predictions of test dataset, and prediction of training set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([103015.2 , 132582.28, 132447.74,  71976.1 , 178537.48, 116161.24,\n",
       "        67851.69,  98791.73, 113969.44, 167921.07])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "y_pred.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.9501847627493607\n",
      "Test Score:  0.9347068473282966\n"
     ]
    }
   ],
   "source": [
    "print('Train Score: ', regressor.score(x_train, y_train))\n",
    "print('Test Score: ', regressor.score(x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The above score tells that our model is 95% accurate with the training dataset and 93% accurate with the test dataset.</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Elimination\n",
    "Backward elimination is a feature selection technique while building a machine learning model. It is used to remove those features that do not have a significant effect on the dependent variable or prediction of output.\n",
    "#### Steps of Backward Elimination\n",
    "Below are some main steps which are used to apply backward elimination process:<br>\n",
    "<b>Step-1:</b> Firstly, We need to select a significance level to stay in the model. (SL=0.05)<br>\n",
    "<b>Step-2:</b> Fit the complete model with all possible predictors/independent variables.<br>\n",
    "<b>Step-3:</b> Choose the predictor which has the highest P-value, such that.<br>\n",
    "<ul>\n",
    "<li>a. If P-value >SL, go to step 4.</li>\n",
    "<li>b. Else Finish, and Our model is ready.</li></ul>\n",
    "<b>Step-4:</b> Remove that predictor.<br>\n",
    "<b>Step-5:</b> Rebuild and fit the model with the remaining variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>*We will use the same model which we build in the previous chapter of MLR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step: 1- Preparation of Backward Elimination</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0.0, 1.0, 165349.2, 136897.8, 471784.1],\n",
       "       [1, 0.0, 0.0, 162597.7, 151377.59, 443898.53],\n",
       "       [1, 1.0, 0.0, 153441.51, 101145.55, 407934.54],\n",
       "       [1, 0.0, 1.0, 144372.41, 118671.85, 383199.62],\n",
       "       [1, 1.0, 0.0, 142107.34, 91391.77, 366168.42],\n",
       "       [1, 0.0, 1.0, 131876.9, 99814.71, 362861.36],\n",
       "       [1, 0.0, 0.0, 134615.46, 147198.87, 127716.82],\n",
       "       [1, 1.0, 0.0, 130298.13, 145530.06, 323876.68],\n",
       "       [1, 0.0, 1.0, 120542.52, 148718.95, 311613.29],\n",
       "       [1, 0.0, 0.0, 123334.88, 108679.17, 304981.62],\n",
       "       [1, 1.0, 0.0, 101913.08, 110594.11, 229160.95],\n",
       "       [1, 0.0, 0.0, 100671.96, 91790.61, 249744.55],\n",
       "       [1, 1.0, 0.0, 93863.75, 127320.38, 249839.44],\n",
       "       [1, 0.0, 0.0, 91992.39, 135495.07, 252664.93],\n",
       "       [1, 1.0, 0.0, 119943.24, 156547.42, 256512.92],\n",
       "       [1, 0.0, 1.0, 114523.61, 122616.84, 261776.23],\n",
       "       [1, 0.0, 0.0, 78013.11, 121597.55, 264346.06],\n",
       "       [1, 0.0, 1.0, 94657.16, 145077.58, 282574.31],\n",
       "       [1, 1.0, 0.0, 91749.16, 114175.79, 294919.57],\n",
       "       [1, 0.0, 1.0, 86419.7, 153514.11, 0.0],\n",
       "       [1, 0.0, 0.0, 76253.86, 113867.3, 298664.47],\n",
       "       [1, 0.0, 1.0, 78389.47, 153773.43, 299737.29],\n",
       "       [1, 1.0, 0.0, 73994.56, 122782.75, 303319.26],\n",
       "       [1, 1.0, 0.0, 67532.53, 105751.03, 304768.73],\n",
       "       [1, 0.0, 1.0, 77044.01, 99281.34, 140574.81],\n",
       "       [1, 0.0, 0.0, 64664.71, 139553.16, 137962.62],\n",
       "       [1, 1.0, 0.0, 75328.87, 144135.98, 134050.07],\n",
       "       [1, 0.0, 1.0, 72107.6, 127864.55, 353183.81],\n",
       "       [1, 1.0, 0.0, 66051.52, 182645.56, 118148.2],\n",
       "       [1, 0.0, 1.0, 65605.48, 153032.06, 107138.38],\n",
       "       [1, 1.0, 0.0, 61994.48, 115641.28, 91131.24],\n",
       "       [1, 0.0, 1.0, 61136.38, 152701.92, 88218.23],\n",
       "       [1, 0.0, 0.0, 63408.86, 129219.61, 46085.25],\n",
       "       [1, 1.0, 0.0, 55493.95, 103057.49, 214634.81],\n",
       "       [1, 0.0, 0.0, 46426.07, 157693.92, 210797.67],\n",
       "       [1, 0.0, 1.0, 46014.02, 85047.44, 205517.64],\n",
       "       [1, 1.0, 0.0, 28663.76, 127056.21, 201126.82],\n",
       "       [1, 0.0, 0.0, 44069.95, 51283.14, 197029.42],\n",
       "       [1, 0.0, 1.0, 20229.59, 65947.93, 185265.1],\n",
       "       [1, 0.0, 0.0, 38558.51, 82982.09, 174999.3],\n",
       "       [1, 0.0, 0.0, 28754.33, 118546.05, 172795.67],\n",
       "       [1, 1.0, 0.0, 27892.92, 84710.77, 164470.71],\n",
       "       [1, 0.0, 0.0, 23640.93, 96189.63, 148001.11],\n",
       "       [1, 0.0, 1.0, 15505.73, 127382.3, 35534.17],\n",
       "       [1, 0.0, 0.0, 22177.74, 154806.14, 28334.72],\n",
       "       [1, 0.0, 1.0, 1000.23, 124153.04, 1903.93],\n",
       "       [1, 1.0, 0.0, 1315.46, 115816.21, 297114.46],\n",
       "       [1, 0.0, 0.0, 0.0, 135426.92, 0.0],\n",
       "       [1, 0.0, 1.0, 542.05, 51743.15, 0.0],\n",
       "       [1, 0.0, 0.0, 0.0, 116983.8, 45173.06]], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as smf # it is used for the estimation of various statistical models such as OLS\n",
    "\n",
    "# adding a column in matrix of features\n",
    "x = np.append(arr = np.ones((50,1)).astype(int), values=x, axis=1)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step: 2:</b>\n",
    "<ul>\n",
    "<li>Now, we are actually going to apply a backward elimination process. Firstly we will create a new feature vector x_opt, which will only contain a set of independent features that are significantly affecting the dependent variable.</li>\n",
    "<li>Next, as per the Backward Elimination process, we need to choose a significant level(0.5), and then need to fit the model with all possible predictors. So for fitting the model, we will create a regressor_OLS object of new class OLS of statsmodels library. Then we will fit it by using the fit() method.</li>\n",
    "<li>Next we need p-value to compare with SL value, so for this we will use summary() method to get the summary table of all the values.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   169.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 05 Aug 2023</td> <th>  Prob (F-statistic):</th> <td>1.34e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:18:32</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>   1074.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.013e+04</td> <td> 6884.820</td> <td>    7.281</td> <td> 0.000</td> <td> 3.62e+04</td> <td>  6.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  198.7888</td> <td> 3371.007</td> <td>    0.059</td> <td> 0.953</td> <td>-6595.030</td> <td> 6992.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>  -41.8870</td> <td> 3256.039</td> <td>   -0.013</td> <td> 0.990</td> <td>-6604.003</td> <td> 6520.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.369</td> <td> 0.000</td> <td>    0.712</td> <td>    0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.517</td> <td> 0.608</td> <td>   -0.132</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.574</td> <td> 0.123</td> <td>   -0.008</td> <td>    0.062</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.782</td> <th>  Durbin-Watson:     </th> <td>   1.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.41e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.572</td> <th>  Cond. No.          </th> <td>1.45e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.45e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     169.9\n",
       "Date:                Sat, 05 Aug 2023   Prob (F-statistic):           1.34e-27\n",
       "Time:                        13:18:32   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1063.\n",
       "Df Residuals:                      44   BIC:                             1074.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\n",
       "x1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\n",
       "x2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\n",
       "x3             0.8060      0.046     17.369      0.000       0.712       0.900\n",
       "x4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\n",
       "x5             0.0270      0.017      1.574      0.123      -0.008       0.062\n",
       "==============================================================================\n",
       "Omnibus:                       14.782   Durbin-Watson:                   1.283\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\n",
       "Skew:                          -0.948   Prob(JB):                     2.41e-05\n",
       "Kurtosis:                       5.572   Cond. No.                     1.45e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.45e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_opt=x [:, [0,1,2,3,4,5]]\n",
    "x_opt = np.array(x_opt, dtype = float)\n",
    "regressor_OLS=smf.OLS(endog = y, exog=x_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table, we will choose the highest p-value, which is for x1=0.953 Now, we have the highest p-value which is greater than the SL value, so will remove the x1 variable (dummy variable) from the table and will refit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   217.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 05 Aug 2023</td> <th>  Prob (F-statistic):</th> <td>8.50e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:18:32</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1061.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    45</td>      <th>  BIC:               </th> <td>   1070.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.018e+04</td> <td> 6747.623</td> <td>    7.437</td> <td> 0.000</td> <td> 3.66e+04</td> <td> 6.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> -136.5042</td> <td> 2801.719</td> <td>   -0.049</td> <td> 0.961</td> <td>-5779.456</td> <td> 5506.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.8059</td> <td>    0.046</td> <td>   17.571</td> <td> 0.000</td> <td>    0.714</td> <td>    0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0269</td> <td>    0.052</td> <td>   -0.521</td> <td> 0.605</td> <td>   -0.131</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0271</td> <td>    0.017</td> <td>    1.625</td> <td> 0.111</td> <td>   -0.007</td> <td>    0.061</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.892</td> <th>  Durbin-Watson:     </th> <td>   1.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.665</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.949</td> <th>  Prob(JB):          </th> <td>1.97e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.608</td> <th>  Cond. No.          </th> <td>1.43e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.43e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.946\n",
       "Method:                 Least Squares   F-statistic:                     217.2\n",
       "Date:                Sat, 05 Aug 2023   Prob (F-statistic):           8.50e-29\n",
       "Time:                        13:18:32   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1061.\n",
       "Df Residuals:                      45   BIC:                             1070.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.018e+04   6747.623      7.437      0.000    3.66e+04    6.38e+04\n",
       "x1          -136.5042   2801.719     -0.049      0.961   -5779.456    5506.447\n",
       "x2             0.8059      0.046     17.571      0.000       0.714       0.898\n",
       "x3            -0.0269      0.052     -0.521      0.605      -0.131       0.077\n",
       "x4             0.0271      0.017      1.625      0.111      -0.007       0.061\n",
       "==============================================================================\n",
       "Omnibus:                       14.892   Durbin-Watson:                   1.284\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.665\n",
       "Skew:                          -0.949   Prob(JB):                     1.97e-05\n",
       "Kurtosis:                       5.608   Cond. No.                     1.43e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.43e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_opt=x[:, [0,2,3,4,5]]\n",
    "x_opt = np.array(x_opt, dtype = float)\n",
    "regressor_OLS=smf.OLS(endog = y, exog=x_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the output image, now five variables remain. In these variables, the highest p-value is 0.961. So we will remove it in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   296.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 05 Aug 2023</td> <th>  Prob (F-statistic):</th> <td>4.53e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:18:33</td>     <th>  Log-Likelihood:    </th> <td> -525.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th> <td>   1066.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.012e+04</td> <td> 6572.353</td> <td>    7.626</td> <td> 0.000</td> <td> 3.69e+04</td> <td> 6.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.8057</td> <td>    0.045</td> <td>   17.846</td> <td> 0.000</td> <td>    0.715</td> <td>    0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.0268</td> <td>    0.051</td> <td>   -0.526</td> <td> 0.602</td> <td>   -0.130</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0272</td> <td>    0.016</td> <td>    1.655</td> <td> 0.105</td> <td>   -0.006</td> <td>    0.060</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.838</td> <th>  Durbin-Watson:     </th> <td>   1.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.949</td> <th>  Prob(JB):          </th> <td>2.21e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.586</td> <th>  Cond. No.          </th> <td>1.40e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.948\n",
       "Method:                 Least Squares   F-statistic:                     296.0\n",
       "Date:                Sat, 05 Aug 2023   Prob (F-statistic):           4.53e-30\n",
       "Time:                        13:18:33   Log-Likelihood:                -525.39\n",
       "No. Observations:                  50   AIC:                             1059.\n",
       "Df Residuals:                      46   BIC:                             1066.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\n",
       "x1             0.8057      0.045     17.846      0.000       0.715       0.897\n",
       "x2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\n",
       "x3             0.0272      0.016      1.655      0.105      -0.006       0.060\n",
       "==============================================================================\n",
       "Omnibus:                       14.838   Durbin-Watson:                   1.282\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\n",
       "Skew:                          -0.949   Prob(JB):                     2.21e-05\n",
       "Kurtosis:                       5.586   Cond. No.                     1.40e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_opt=x[:, [0,3,4,5]]\n",
    "x_opt = np.array(x_opt, dtype = float)\n",
    "regressor_OLS=smf.OLS(endog = y, exog=x_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output image, we can see the dummy variable(x2) has been removed. And the next highest value is .602, which is still greater than .5, so we need to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   450.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 05 Aug 2023</td> <th>  Prob (F-statistic):</th> <td>2.16e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:18:33</td>     <th>  Log-Likelihood:    </th> <td> -525.54</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1057.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    47</td>      <th>  BIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.698e+04</td> <td> 2689.933</td> <td>   17.464</td> <td> 0.000</td> <td> 4.16e+04</td> <td> 5.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.7966</td> <td>    0.041</td> <td>   19.266</td> <td> 0.000</td> <td>    0.713</td> <td>    0.880</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0299</td> <td>    0.016</td> <td>    1.927</td> <td> 0.060</td> <td>   -0.001</td> <td>    0.061</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.677</td> <th>  Durbin-Watson:     </th> <td>   1.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.939</td> <th>  Prob(JB):          </th> <td>2.54e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.575</td> <th>  Cond. No.          </th> <td>5.32e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.32e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.950\n",
       "Model:                            OLS   Adj. R-squared:                  0.948\n",
       "Method:                 Least Squares   F-statistic:                     450.8\n",
       "Date:                Sat, 05 Aug 2023   Prob (F-statistic):           2.16e-31\n",
       "Time:                        13:18:33   Log-Likelihood:                -525.54\n",
       "No. Observations:                  50   AIC:                             1057.\n",
       "Df Residuals:                      47   BIC:                             1063.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\n",
       "x1             0.7966      0.041     19.266      0.000       0.713       0.880\n",
       "x2             0.0299      0.016      1.927      0.060      -0.001       0.061\n",
       "==============================================================================\n",
       "Omnibus:                       14.677   Durbin-Watson:                   1.257\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\n",
       "Skew:                          -0.939   Prob(JB):                     2.54e-05\n",
       "Kurtosis:                       5.575   Cond. No.                     5.32e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.32e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_opt=x[:, [0,3,5]]\n",
    "x_opt = np.array(x_opt, dtype = float)\n",
    "regressor_OLS=smf.OLS(endog = y, exog=x_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above output image, the variable (Admin spend) has been removed. But still, there is one variable left, which is marketing spend as it has a high p-value (0.60). So we need to remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   849.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 05 Aug 2023</td> <th>  Prob (F-statistic):</th> <td>3.50e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:18:33</td>     <th>  Log-Likelihood:    </th> <td> -527.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    48</td>      <th>  BIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.903e+04</td> <td> 2537.897</td> <td>   19.320</td> <td> 0.000</td> <td> 4.39e+04</td> <td> 5.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.8543</td> <td>    0.029</td> <td>   29.151</td> <td> 0.000</td> <td>    0.795</td> <td>    0.913</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13.727</td> <th>  Durbin-Watson:     </th> <td>   1.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  18.536</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.911</td> <th>  Prob(JB):          </th> <td>9.44e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.361</td> <th>  Cond. No.          </th> <td>1.65e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.65e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.947\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     849.8\n",
       "Date:                Sat, 05 Aug 2023   Prob (F-statistic):           3.50e-32\n",
       "Time:                        13:18:33   Log-Likelihood:                -527.44\n",
       "No. Observations:                  50   AIC:                             1059.\n",
       "Df Residuals:                      48   BIC:                             1063.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\n",
       "x1             0.8543      0.029     29.151      0.000       0.795       0.913\n",
       "==============================================================================\n",
       "Omnibus:                       13.727   Durbin-Watson:                   1.116\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\n",
       "Skew:                          -0.911   Prob(JB):                     9.44e-05\n",
       "Kurtosis:                       5.361   Cond. No.                     1.65e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.65e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_opt=x[:, [0,3]]\n",
    "x_opt = np.array(x_opt, dtype = float)\n",
    "regressor_OLS=smf.OLS(endog = y, exog=x_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>*Note: </b><i>Thus only the R&D independent variable is a significant variable for the prediction. So we can now predict efficiently using this variable.</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "<ul>\n",
    "<li>Ridge regression is one of the most robust versions of linear regression in which a small amount of bias is introduced so that we can get better long term predictions.</li>\n",
    "<li>The amount of bias added to the model is known as Ridge Regression penalty. We can compute this penalty term by multiplying with the lambda to the squared weight of each individual features.</li>\n",
    "<li>The equation for ridge regression will be:</li>\n",
    "\n",
    "$L(x,y) = Min(\\sum \\limits_{i=1}^{n}(y_{i} - w_{i}x_{i})^2 + \\lambda \\sum \\limits_{i=1}^{n}(w_{i})^2)$\n",
    "<li>A general linear or polynomial regression will fail if there is high collinearity between the independent variables, so to solve such problems, Ridge regression can be used.</li>\n",
    "<li>Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization.</li>\n",
    "<li>It helps to solve the problems if we have more parameters than samples.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step 1: Data Pre-processing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data to be trained and tested\n",
    "teams = pd.read_csv(\"teams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting teams data into train and test data\n",
    "train, test = train_test_split(teams, test_size=0.2, random_state=1)\n",
    "\n",
    "## defining predictor(independent variables) and target(dependent variable)\n",
    "predictors = [\"athletes\", \"events\"]\n",
    "target = \"medals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      "       athletes  events\n",
      "1322         6       6\n",
      "1872       119      80\n",
      "953          4       4\n",
      "1117         2       2\n",
      "1993        43      25\n",
      "...        ...     ...\n",
      "1791        40      25\n",
      "1096        36      23\n",
      "1932       719     245\n",
      "235         13      11\n",
      "1061        50      38\n",
      "\n",
      "[1611 rows x 2 columns] \n",
      "y: \n",
      "       medals\n",
      "1322       0\n",
      "1872       5\n",
      "953        0\n",
      "1117       0\n",
      "1993       0\n",
      "...      ...\n",
      "1791       1\n",
      "1096       1\n",
      "1932     264\n",
      "235        0\n",
      "1061       3\n",
      "\n",
      "[1611 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "X = train[predictors].copy()\n",
    "y = train[[target]].copy()\n",
    "print(\"X: \\n\",X,\"\\ny: \\n\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the X values using mean and standard deviation\n",
    "x_mean = X.mean()\n",
    "x_std = X.std()\n",
    "X = (X - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>athletes</th>\n",
       "      <th>events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.611000e+03</td>\n",
       "      <td>1.611000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.370681e-17</td>\n",
       "      <td>-9.923781e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.768883e-01</td>\n",
       "      <td>-7.143930e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-5.297371e-01</td>\n",
       "      <td>-6.123079e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-4.197174e-01</td>\n",
       "      <td>-4.489717e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-2.679027e-02</td>\n",
       "      <td>1.839560e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.008571e+00</td>\n",
       "      <td>4.634867e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           athletes        events\n",
       "count  1.611000e+03  1.611000e+03\n",
       "mean  -2.370681e-17 -9.923781e-18\n",
       "std    1.000000e+00  1.000000e+00\n",
       "min   -5.768883e-01 -7.143930e-01\n",
       "25%   -5.297371e-01 -6.123079e-01\n",
       "50%   -4.197174e-01 -4.489717e-01\n",
       "75%   -2.679027e-02  1.839560e-01\n",
       "max    6.008571e+00  4.634867e+00"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add an intercept term into X matrix to calculate the y-intercept coefficient\n",
    "X[\"intercept\"] = 1\n",
    "X = X[[\"intercept\"] + predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1322</th>\n",
       "      <th>1872</th>\n",
       "      <th>953</th>\n",
       "      <th>1117</th>\n",
       "      <th>1993</th>\n",
       "      <th>385</th>\n",
       "      <th>1287</th>\n",
       "      <th>1831</th>\n",
       "      <th>0</th>\n",
       "      <th>1159</th>\n",
       "      <th>...</th>\n",
       "      <th>960</th>\n",
       "      <th>847</th>\n",
       "      <th>1669</th>\n",
       "      <th>715</th>\n",
       "      <th>905</th>\n",
       "      <th>1791</th>\n",
       "      <th>1096</th>\n",
       "      <th>1932</th>\n",
       "      <th>235</th>\n",
       "      <th>1061</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>athletes</th>\n",
       "      <td>-0.537596</td>\n",
       "      <td>0.350420</td>\n",
       "      <td>-0.553313</td>\n",
       "      <td>-0.569030</td>\n",
       "      <td>-0.246829</td>\n",
       "      <td>-0.482586</td>\n",
       "      <td>-0.537596</td>\n",
       "      <td>0.138239</td>\n",
       "      <td>-0.521879</td>\n",
       "      <td>-0.152527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.199678</td>\n",
       "      <td>-0.160386</td>\n",
       "      <td>-0.529737</td>\n",
       "      <td>-0.529737</td>\n",
       "      <td>-0.341132</td>\n",
       "      <td>-0.270405</td>\n",
       "      <td>-0.301839</td>\n",
       "      <td>5.065546</td>\n",
       "      <td>-0.482586</td>\n",
       "      <td>-0.191820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>events</th>\n",
       "      <td>-0.612308</td>\n",
       "      <td>0.898552</td>\n",
       "      <td>-0.653142</td>\n",
       "      <td>-0.693976</td>\n",
       "      <td>-0.224384</td>\n",
       "      <td>-0.571474</td>\n",
       "      <td>-0.612308</td>\n",
       "      <td>0.102288</td>\n",
       "      <td>-0.571474</td>\n",
       "      <td>-0.163133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285636</td>\n",
       "      <td>-0.101882</td>\n",
       "      <td>-0.612308</td>\n",
       "      <td>-0.591891</td>\n",
       "      <td>-0.367304</td>\n",
       "      <td>-0.224384</td>\n",
       "      <td>-0.265219</td>\n",
       "      <td>4.267361</td>\n",
       "      <td>-0.510223</td>\n",
       "      <td>0.041037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1611 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1322      1872      953       1117      1993      385   \\\n",
       "intercept  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "athletes  -0.537596  0.350420 -0.553313 -0.569030 -0.246829 -0.482586   \n",
       "events    -0.612308  0.898552 -0.653142 -0.693976 -0.224384 -0.571474   \n",
       "\n",
       "               1287      1831      0         1159  ...      960       847   \\\n",
       "intercept  1.000000  1.000000  1.000000  1.000000  ...  1.000000  1.000000   \n",
       "athletes  -0.537596  0.138239 -0.521879 -0.152527  ... -0.199678 -0.160386   \n",
       "events    -0.612308  0.102288 -0.571474 -0.163133  ... -0.285636 -0.101882   \n",
       "\n",
       "               1669      715       905       1791      1096      1932  \\\n",
       "intercept  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "athletes  -0.529737 -0.529737 -0.341132 -0.270405 -0.301839  5.065546   \n",
       "events    -0.612308 -0.591891 -0.367304 -0.224384 -0.265219  4.267361   \n",
       "\n",
       "               235       1061  \n",
       "intercept  1.000000  1.000000  \n",
       "athletes  -0.482586 -0.191820  \n",
       "events    -0.510223  0.041037  \n",
       "\n",
       "[3 rows x 1611 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above data, intercept is $b_{0}$, athletes in $b_{1}$ and events is $b_{2}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step 2: Fitting your Ridge regression model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0.],\n",
       "       [0., 2., 0.],\n",
       "       [0., 0., 2.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating our penalty matrix\n",
    "alpha = 2\n",
    "I = np.identity(X.shape[1])\n",
    "penalty = alpha * I\n",
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 2., 0.],\n",
       "       [0., 0., 2.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty[0][0] = 0 # this is done because we don't want to penalize the y-intercept\n",
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>10.691496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>athletes</th>\n",
       "      <td>61.857734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>events</th>\n",
       "      <td>-34.632920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              medals\n",
       "intercept  10.691496\n",
       "athletes   61.857734\n",
       "events    -34.632920"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Equation to solve for B\n",
    "B = np.linalg.inv(X.T @ X + penalty) @ X.T @ y # '@' for matrix multiplication\n",
    "B.index = [\"intercept\", \"athletes\", \"events\"]\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_X: \n",
      "       intercept  athletes    events\n",
      "309           1 -0.553313 -0.653142\n",
      "285           1  0.594035  1.000637\n",
      "919           1 -0.144668  0.102288\n",
      "120           1  0.146098  0.531045\n",
      "585           1 -0.301839 -0.122299\n",
      "...         ...       ...       ...\n",
      "541           1 -0.380425 -0.408138\n",
      "1863          1 -0.191820  0.143122\n",
      "622           1 -0.058224  0.388126\n",
      "1070          1 -0.569030 -0.693976\n",
      "1196          1 -0.553313 -0.653142\n",
      "\n",
      "[403 rows x 3 columns] \n",
      "predictions: \n",
      "          medals\n",
      "309   -0.914959\n",
      "285   12.782156\n",
      "919   -1.799893\n",
      "120    1.337116\n",
      "585   -3.744014\n",
      "...         ...\n",
      "541    1.294285\n",
      "1863  -6.130765\n",
      "622   -6.352080\n",
      "1070  -0.472980\n",
      "1196  -0.914959\n",
      "\n",
      "[403 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# doing the same for \n",
    "test_X = test[predictors]\n",
    "test_X = (test_X - x_mean) / x_std\n",
    "test_X[\"intercept\"] = 1\n",
    "test_X = test_X[[\"intercept\"] + predictors]\n",
    "\n",
    "predictions = test_X @ B\n",
    "print(\"test_X: \\n\",test_X, \"\\npredictions: \\n\",predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_fit(train, predictors, target, alpha):\n",
    "    X = train[predictors].copy()\n",
    "    y = train[[target]].copy()\n",
    "    \n",
    "    x_mean = X.mean()\n",
    "    x_std = X.std()\n",
    "    \n",
    "    X = (X - x_mean) / x_std\n",
    "    X[\"intercept\"] = 1\n",
    "    X = X[[\"intercept\"] + predictors]\n",
    "    \n",
    "    penalty = alpha * np.identity(X.shape[1])\n",
    "    penalty[0][0] = 0\n",
    "    \n",
    "    B = np.linalg.inv(X.T @ X + penalty) @ X.T @ y\n",
    "    B.index = [\"intercept\", \"athletes\", \"events\"]\n",
    "    return B, x_mean, x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, x_mean, x_std = ridge_fit(train, predictors, target, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_predict(test, predictors, x_mean, x_std, B):\n",
    "    test_X = test[predictors]\n",
    "    test_X = (test_X - x_mean) / x_std\n",
    "    test_X[\"intercept\"] = 1\n",
    "    test_X = test_X[[\"intercept\"] + predictors]\n",
    "\n",
    "    predictions = test_X @ B\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ridge_predict(test, predictors, x_mean, x_std, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge(alpha=2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(alpha=alpha)\n",
    "ridge.fit(X[predictors], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.85773366 -34.63292036]] \n",
      "\n",
      "[10.69149597]\n"
     ]
    }
   ],
   "source": [
    "print(ridge.coef_,\"\\n\")\n",
    "print(ridge.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>8.348877e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>-3.534950e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>-2.051692e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>-3.286260e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>-1.483258e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>2.309264e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>-2.797762e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>-3.730349e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>1.048051e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>8.348877e-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            medals\n",
       "309   8.348877e-14\n",
       "285  -3.534950e-13\n",
       "919  -2.051692e-13\n",
       "120  -3.286260e-13\n",
       "585  -1.483258e-13\n",
       "...            ...\n",
       "541   2.309264e-14\n",
       "1863 -2.797762e-13\n",
       "622  -3.730349e-13\n",
       "1070  1.048051e-13\n",
       "1196  8.348877e-14\n",
       "\n",
       "[403 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_predictions = ridge.predict(test_X[predictors])\n",
    "predictions - sklearn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.309640830161113,\n",
       " 6.306044331952916,\n",
       " 6.272283376431602,\n",
       " 6.114051204717718,\n",
       " 7.156811236590466,\n",
       " 6.9780545895757315]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = []\n",
    "alphas = [10**i for i in range(-2,4)]\n",
    "\n",
    "for alpha in alphas:\n",
    "    B, x_mean, x_std = ridge_fit(train, predictors, target, alpha)\n",
    "    predictions = ridge_predict(test, predictors, x_mean, x_std, B)\n",
    "    \n",
    "    errors.append(mean_absolute_error(test[target], predictions))\n",
    "\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01, 0.1, 1, 10, 100, 1000]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
