{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "#### y = $a_{0} + a_{1}x + ε$\n",
    "y = Dependent Variable (Target Variable)<br>x = Independent Variable (predictor Variable)<br>\n",
    "$a_{0}$ = intercept of the line (Gives an additional degree of freedom)<br>\n",
    "$a_{1}$ = Linear regression coefficient (scale factor to each input value).<br>ε = random error\n",
    "<br>\n",
    "### Cost function\n",
    "When working with linear regression, our main goal is to find the best fit line that means the error between predicted values and actual values should be minimized. The best fit line will have the least error.The different values for weights or coefficient of lines (a0, a1) gives the different line of regression, and the cost function is used to estimate the values of the coefficient for the best fit line.\n",
    "Cost function optimizes the regression coefficients or weights. It measures how a linear regression model is performing.<br>For Linear Regression, we use the Mean Squared Error (MSE) cost function, which is the average of squared error occurred between the predicted values and actual values. It can be written as:<br>\n",
    "$MSE = \\frac{1}{N} \\sum \\limits_{i=1}^{n}(y_{i} - (a_{1}x_{i} + a_{0}))^2$<br>\n",
    "where, N = Total number of observations<br>\n",
    "$y_{i}$ = Actual value<br>\n",
    "$(a_{1}x_{i} + a_{0})$ = Predicted Value\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "Algorithm that models the relationship between a dependent variable and a single independent variable. The relationship shown by a Simple Linear Regression model is linear or a sloped straight line, hence it is called Simple Linear Regression.<br>\n",
    "The key point in Simple Linear Regression is that the <b><i>dependent variable must be a continuous/real value</i></b>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Problem Statement example for Simple Linear Regression:</b><br>\n",
    "Here we are taking a dataset that has two variables: <b>salary (dependent variable)</b> and <b>experience (Independent variable)</b>.<br>The goals of this problem is:\n",
    "<ul>\n",
    "<li>We want to find out if there is any correlation between these two variables</li>\n",
    "<li>We will find the best fit line for the dataset.</li>\n",
    "<li>How the dependent variable is changing by changing the independent variable.</li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for managing data\n",
    "import matplotlib.pyplot as plt # for plotting the graph\n",
    "import pandas as pd # for reading data\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset into training and test set\n",
    "from sklearn.linear_model import LinearRegression #for fitting the Simple Linear Regression model to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearsExperience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>39343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3</td>\n",
       "      <td>46205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>37731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>43525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2</td>\n",
       "      <td>39891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.9</td>\n",
       "      <td>56642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>60150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.2</td>\n",
       "      <td>54445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.2</td>\n",
       "      <td>64445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.7</td>\n",
       "      <td>57189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.9</td>\n",
       "      <td>63218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.0</td>\n",
       "      <td>55794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.0</td>\n",
       "      <td>56957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.1</td>\n",
       "      <td>57081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.5</td>\n",
       "      <td>61111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.9</td>\n",
       "      <td>67938.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.1</td>\n",
       "      <td>66029.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.3</td>\n",
       "      <td>83088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.9</td>\n",
       "      <td>81363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.0</td>\n",
       "      <td>93940.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.8</td>\n",
       "      <td>91738.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.1</td>\n",
       "      <td>98273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.9</td>\n",
       "      <td>101302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.2</td>\n",
       "      <td>113812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8.7</td>\n",
       "      <td>109431.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.0</td>\n",
       "      <td>105582.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.5</td>\n",
       "      <td>116969.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.6</td>\n",
       "      <td>112635.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.3</td>\n",
       "      <td>122391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.5</td>\n",
       "      <td>121872.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YearsExperience    Salary\n",
       "0               1.1   39343.0\n",
       "1               1.3   46205.0\n",
       "2               1.5   37731.0\n",
       "3               2.0   43525.0\n",
       "4               2.2   39891.0\n",
       "5               2.9   56642.0\n",
       "6               3.0   60150.0\n",
       "7               3.2   54445.0\n",
       "8               3.2   64445.0\n",
       "9               3.7   57189.0\n",
       "10              3.9   63218.0\n",
       "11              4.0   55794.0\n",
       "12              4.0   56957.0\n",
       "13              4.1   57081.0\n",
       "14              4.5   61111.0\n",
       "15              4.9   67938.0\n",
       "16              5.1   66029.0\n",
       "17              5.3   83088.0\n",
       "18              5.9   81363.0\n",
       "19              6.0   93940.0\n",
       "20              6.8   91738.0\n",
       "21              7.1   98273.0\n",
       "22              7.9  101302.0\n",
       "23              8.2  113812.0\n",
       "24              8.7  109431.0\n",
       "25              9.0  105582.0\n",
       "26              9.5  116969.0\n",
       "27              9.6  112635.0\n",
       "28             10.3  122391.0\n",
       "29             10.5  121872.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading dataset into our code\n",
    "data = pd.read_csv('Salary_Data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " [[ 1.1]\n",
      " [ 1.3]\n",
      " [ 1.5]\n",
      " [ 2. ]\n",
      " [ 2.2]\n",
      " [ 2.9]\n",
      " [ 3. ]\n",
      " [ 3.2]\n",
      " [ 3.2]\n",
      " [ 3.7]\n",
      " [ 3.9]\n",
      " [ 4. ]\n",
      " [ 4. ]\n",
      " [ 4.1]\n",
      " [ 4.5]\n",
      " [ 4.9]\n",
      " [ 5.1]\n",
      " [ 5.3]\n",
      " [ 5.9]\n",
      " [ 6. ]\n",
      " [ 6.8]\n",
      " [ 7.1]\n",
      " [ 7.9]\n",
      " [ 8.2]\n",
      " [ 8.7]\n",
      " [ 9. ]\n",
      " [ 9.5]\n",
      " [ 9.6]\n",
      " [10.3]\n",
      " [10.5]] \n",
      "y: \n",
      " [ 39343.  46205.  37731.  43525.  39891.  56642.  60150.  54445.  64445.\n",
      "  57189.  63218.  55794.  56957.  57081.  61111.  67938.  66029.  83088.\n",
      "  81363.  93940.  91738.  98273. 101302. 113812. 109431. 105582. 116969.\n",
      " 112635. 122391. 121872.]\n"
     ]
    }
   ],
   "source": [
    "# we need to extract the dependent(salary) and independent(years of experience) variables from the given dataset\n",
    "x = data.iloc[:, :-1].values ## iloc[] is used to extract the required rows and columns from the dataset\n",
    "y = data.iloc[:, 1].values\n",
    "print(\"x: \\n\",x,\"\\ny: \\n\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will split both variables into the test set and training set\n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 1/3, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 2: Fitting the Simple Linear Regression to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have fitted our regressor object to the training set so that the model can easily learn \n",
    "# the correlations between the predictor and target variables.\n",
    "regressor= LinearRegression()\n",
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 3: Prediction of test result</b>\n",
    "In this step, we will provide the test dataset (new observations) to the model to check whether it can predict the correct output or not.<br>\n",
    "We will create a prediction vector y_pred, and x_pred, which will contain predictions of test dataset, and prediction of training set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegression' object has no attribute 'R2_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12908/2998159016.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mR2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'LinearRegression' object has no attribute 'R2_score'"
     ]
    }
   ],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "x_pred = regressor.predict(x_train)\n",
    "print(regressor.R2_score(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 4: Visualizing the Training set results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_train, y_train, color=\"orange\")\n",
    "plt.plot(x_train, x_pred, color=\"magenta\")\n",
    "plt.title(\"Salary vs Experience (Training Dataset)\")\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary(In Rupees)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, we can see the real values observations in orange dots and predicted values are covered by the magenta regression line. The regression line shows a correlation between the dependent and independent variable.\n",
    "The good fit of the line can be observed by calculating the difference between actual values and predicted values. But as we can see in the above plot, most of the observations are close to the regression line, hence our model is good for the training set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 5: Visualizing the Test set results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_test, y_test, color=\"blue\")\n",
    "plt.plot(x_train, x_pred, color=\"magenta\")\n",
    "plt.title(\"Salary vs Experience (Test Dataset)\")\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary(In Rupees)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, there are observations given by the blue color, and prediction is given by the magenta regression line. As we can see, most of the observations are close to the regression line, hence we can say our Simple Linear Regression is a good model and able to make good predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "This algorithm is one of the important regression algorithms which models the linear relationship between a single dependent continuous variable and more than one independent variable.\n",
    "<ul>\n",
    "<li>For MLR, the dependent or target variable(Y) must be the continuous/real, but the predictor or independent variable may be of continuous or categorical form.</li>\n",
    "<li>Each feature variable must model the linear relationship with the dependent variable.</li>\n",
    "<li>MLR tries to fit a regression line through a multidimensional space of data-points.</li>\n",
    "</ul>\n",
    "\n",
    "$y = b_{0}+b_{1}x_{1}+ b_{2}x_{2}+ b_{3}x_{3}+...... b_{n}x_{n}$\n",
    "<br>Where,\n",
    "\n",
    "$y$ = Output/Response variable<br>\n",
    "$b_{0}, b_{1}, b_{2}, b_{3},...., b_{n}$ = Coefficients of the model.<br>\n",
    "$x_{1}, x_{2}, x_{3}, x_{4},...$ = Various Independent/feature variable<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Problem Description:</b><br>\n",
    "We have a dataset of 50 start-up companies. This dataset contains five main information: R&D Spend, Administration Spend, Marketing Spend, State, and Profit for a financial year. Our goal is to create a model that can easily determine which company has a maximum profit, and which is the most affecting factor for the profit of a company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for managing data\n",
    "import matplotlib.pyplot as plt # for plotting the graph\n",
    "import pandas as pd # for reading data\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset into training and test set\n",
    "from sklearn.linear_model import LinearRegression #for fitting the Simple Linear Regression model to the training dataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '50_Startups.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12908/3503611008.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Loading dataset into our code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'50_Startups.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91983\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91983\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91983\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91983\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91983\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91983\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91983\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91983\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '50_Startups.csv'"
     ]
    }
   ],
   "source": [
    "## Loading dataset into our code\n",
    "data = pd.read_csv('50_Startups.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " [[165349.2 136897.8 471784.1 'New York']\n",
      " [162597.7 151377.59 443898.53 'California']\n",
      " [153441.51 101145.55 407934.54 'Florida']\n",
      " [144372.41 118671.85 383199.62 'New York']\n",
      " [142107.34 91391.77 366168.42 'Florida']\n",
      " [131876.9 99814.71 362861.36 'New York']\n",
      " [134615.46 147198.87 127716.82 'California']\n",
      " [130298.13 145530.06 323876.68 'Florida']\n",
      " [120542.52 148718.95 311613.29 'New York']\n",
      " [123334.88 108679.17 304981.62 'California']\n",
      " [101913.08 110594.11 229160.95 'Florida']\n",
      " [100671.96 91790.61 249744.55 'California']\n",
      " [93863.75 127320.38 249839.44 'Florida']\n",
      " [91992.39 135495.07 252664.93 'California']\n",
      " [119943.24 156547.42 256512.92 'Florida']\n",
      " [114523.61 122616.84 261776.23 'New York']\n",
      " [78013.11 121597.55 264346.06 'California']\n",
      " [94657.16 145077.58 282574.31 'New York']\n",
      " [91749.16 114175.79 294919.57 'Florida']\n",
      " [86419.7 153514.11 0.0 'New York']\n",
      " [76253.86 113867.3 298664.47 'California']\n",
      " [78389.47 153773.43 299737.29 'New York']\n",
      " [73994.56 122782.75 303319.26 'Florida']\n",
      " [67532.53 105751.03 304768.73 'Florida']\n",
      " [77044.01 99281.34 140574.81 'New York']\n",
      " [64664.71 139553.16 137962.62 'California']\n",
      " [75328.87 144135.98 134050.07 'Florida']\n",
      " [72107.6 127864.55 353183.81 'New York']\n",
      " [66051.52 182645.56 118148.2 'Florida']\n",
      " [65605.48 153032.06 107138.38 'New York']\n",
      " [61994.48 115641.28 91131.24 'Florida']\n",
      " [61136.38 152701.92 88218.23 'New York']\n",
      " [63408.86 129219.61 46085.25 'California']\n",
      " [55493.95 103057.49 214634.81 'Florida']\n",
      " [46426.07 157693.92 210797.67 'California']\n",
      " [46014.02 85047.44 205517.64 'New York']\n",
      " [28663.76 127056.21 201126.82 'Florida']\n",
      " [44069.95 51283.14 197029.42 'California']\n",
      " [20229.59 65947.93 185265.1 'New York']\n",
      " [38558.51 82982.09 174999.3 'California']\n",
      " [28754.33 118546.05 172795.67 'California']\n",
      " [27892.92 84710.77 164470.71 'Florida']\n",
      " [23640.93 96189.63 148001.11 'California']\n",
      " [15505.73 127382.3 35534.17 'New York']\n",
      " [22177.74 154806.14 28334.72 'California']\n",
      " [1000.23 124153.04 1903.93 'New York']\n",
      " [1315.46 115816.21 297114.46 'Florida']\n",
      " [0.0 135426.92 0.0 'California']\n",
      " [542.05 51743.15 0.0 'New York']\n",
      " [0.0 116983.8 45173.06 'California']] \n",
      "y: \n",
      " [192261.83 191792.06 191050.39 182901.99 166187.94 156991.12 156122.51\n",
      " 155752.6  152211.77 149759.96 146121.95 144259.4  141585.52 134307.35\n",
      " 132602.65 129917.04 126992.93 125370.37 124266.9  122776.86 118474.03\n",
      " 111313.02 110352.25 108733.99 108552.04 107404.34 105733.54 105008.31\n",
      " 103282.38 101004.64  99937.59  97483.56  97427.84  96778.92  96712.8\n",
      "  96479.51  90708.19  89949.14  81229.06  81005.76  78239.91  77798.83\n",
      "  71498.49  69758.98  65200.33  64926.08  49490.75  42559.73  35673.41\n",
      "  14681.4 ]\n"
     ]
    }
   ],
   "source": [
    "# we need to extract the dependent(profit) and independent(R&D spend, Marketing Spend, Administration and State) variables from the given dataset\n",
    "x = data.iloc[:, :-1].values ## iloc[] is used to extract the required rows and columns from the dataset\n",
    "y = data.iloc[:, 4].values\n",
    "print(\"x: \\n\",x,\"\\ny: \\n\",y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above output, the last column contains categorical variables which are not suitable to apply directly for fitting the model. So we need to encode this variable.To encode the categorical variable into numbers, we will use the LabelEncoder class. But it is not sufficient because it still has some relational order, which may create a wrong model. So in order to remove this problem, we will use OneHotEncoder, which will create the dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 1.0, 165349.2, 136897.8, 471784.1],\n",
       "       [1.0, 0.0, 0.0, 162597.7, 151377.59, 443898.53],\n",
       "       [0.0, 1.0, 0.0, 153441.51, 101145.55, 407934.54],\n",
       "       [0.0, 0.0, 1.0, 144372.41, 118671.85, 383199.62],\n",
       "       [0.0, 1.0, 0.0, 142107.34, 91391.77, 366168.42],\n",
       "       [0.0, 0.0, 1.0, 131876.9, 99814.71, 362861.36],\n",
       "       [1.0, 0.0, 0.0, 134615.46, 147198.87, 127716.82],\n",
       "       [0.0, 1.0, 0.0, 130298.13, 145530.06, 323876.68],\n",
       "       [0.0, 0.0, 1.0, 120542.52, 148718.95, 311613.29],\n",
       "       [1.0, 0.0, 0.0, 123334.88, 108679.17, 304981.62],\n",
       "       [0.0, 1.0, 0.0, 101913.08, 110594.11, 229160.95],\n",
       "       [1.0, 0.0, 0.0, 100671.96, 91790.61, 249744.55],\n",
       "       [0.0, 1.0, 0.0, 93863.75, 127320.38, 249839.44],\n",
       "       [1.0, 0.0, 0.0, 91992.39, 135495.07, 252664.93],\n",
       "       [0.0, 1.0, 0.0, 119943.24, 156547.42, 256512.92],\n",
       "       [0.0, 0.0, 1.0, 114523.61, 122616.84, 261776.23],\n",
       "       [1.0, 0.0, 0.0, 78013.11, 121597.55, 264346.06],\n",
       "       [0.0, 0.0, 1.0, 94657.16, 145077.58, 282574.31],\n",
       "       [0.0, 1.0, 0.0, 91749.16, 114175.79, 294919.57],\n",
       "       [0.0, 0.0, 1.0, 86419.7, 153514.11, 0.0],\n",
       "       [1.0, 0.0, 0.0, 76253.86, 113867.3, 298664.47],\n",
       "       [0.0, 0.0, 1.0, 78389.47, 153773.43, 299737.29],\n",
       "       [0.0, 1.0, 0.0, 73994.56, 122782.75, 303319.26],\n",
       "       [0.0, 1.0, 0.0, 67532.53, 105751.03, 304768.73],\n",
       "       [0.0, 0.0, 1.0, 77044.01, 99281.34, 140574.81],\n",
       "       [1.0, 0.0, 0.0, 64664.71, 139553.16, 137962.62],\n",
       "       [0.0, 1.0, 0.0, 75328.87, 144135.98, 134050.07],\n",
       "       [0.0, 0.0, 1.0, 72107.6, 127864.55, 353183.81],\n",
       "       [0.0, 1.0, 0.0, 66051.52, 182645.56, 118148.2],\n",
       "       [0.0, 0.0, 1.0, 65605.48, 153032.06, 107138.38],\n",
       "       [0.0, 1.0, 0.0, 61994.48, 115641.28, 91131.24],\n",
       "       [0.0, 0.0, 1.0, 61136.38, 152701.92, 88218.23],\n",
       "       [1.0, 0.0, 0.0, 63408.86, 129219.61, 46085.25],\n",
       "       [0.0, 1.0, 0.0, 55493.95, 103057.49, 214634.81],\n",
       "       [1.0, 0.0, 0.0, 46426.07, 157693.92, 210797.67],\n",
       "       [0.0, 0.0, 1.0, 46014.02, 85047.44, 205517.64],\n",
       "       [0.0, 1.0, 0.0, 28663.76, 127056.21, 201126.82],\n",
       "       [1.0, 0.0, 0.0, 44069.95, 51283.14, 197029.42],\n",
       "       [0.0, 0.0, 1.0, 20229.59, 65947.93, 185265.1],\n",
       "       [1.0, 0.0, 0.0, 38558.51, 82982.09, 174999.3],\n",
       "       [1.0, 0.0, 0.0, 28754.33, 118546.05, 172795.67],\n",
       "       [0.0, 1.0, 0.0, 27892.92, 84710.77, 164470.71],\n",
       "       [1.0, 0.0, 0.0, 23640.93, 96189.63, 148001.11],\n",
       "       [0.0, 0.0, 1.0, 15505.73, 127382.3, 35534.17],\n",
       "       [1.0, 0.0, 0.0, 22177.74, 154806.14, 28334.72],\n",
       "       [0.0, 0.0, 1.0, 1000.23, 124153.04, 1903.93],\n",
       "       [0.0, 1.0, 0.0, 1315.46, 115816.21, 297114.46],\n",
       "       [1.0, 0.0, 0.0, 0.0, 135426.92, 0.0],\n",
       "       [0.0, 0.0, 1.0, 542.05, 51743.15, 0.0],\n",
       "       [1.0, 0.0, 0.0, 0.0, 116983.8, 45173.06]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Catgorical data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "labelencoder_x= LabelEncoder()\n",
    "x[:, 3]= labelencoder_x.fit_transform(x[:,3])\n",
    "ct = ColumnTransformer([(\"State\", OneHotEncoder(), [3])], remainder = 'passthrough')\n",
    "x = ct.fit_transform(x)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>*Note:</b> We should not use all the dummy variables at the same time, so it must be 1 less than the total number of dummy variables, else it will create a dummy variable trap. If we do not remove the first dummy variable, then it may introduce multicollinearity in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 165349.2, 136897.8, 471784.1],\n",
       "       [0.0, 0.0, 162597.7, 151377.59, 443898.53],\n",
       "       [1.0, 0.0, 153441.51, 101145.55, 407934.54],\n",
       "       [0.0, 1.0, 144372.41, 118671.85, 383199.62],\n",
       "       [1.0, 0.0, 142107.34, 91391.77, 366168.42],\n",
       "       [0.0, 1.0, 131876.9, 99814.71, 362861.36],\n",
       "       [0.0, 0.0, 134615.46, 147198.87, 127716.82],\n",
       "       [1.0, 0.0, 130298.13, 145530.06, 323876.68],\n",
       "       [0.0, 1.0, 120542.52, 148718.95, 311613.29],\n",
       "       [0.0, 0.0, 123334.88, 108679.17, 304981.62],\n",
       "       [1.0, 0.0, 101913.08, 110594.11, 229160.95],\n",
       "       [0.0, 0.0, 100671.96, 91790.61, 249744.55],\n",
       "       [1.0, 0.0, 93863.75, 127320.38, 249839.44],\n",
       "       [0.0, 0.0, 91992.39, 135495.07, 252664.93],\n",
       "       [1.0, 0.0, 119943.24, 156547.42, 256512.92],\n",
       "       [0.0, 1.0, 114523.61, 122616.84, 261776.23],\n",
       "       [0.0, 0.0, 78013.11, 121597.55, 264346.06],\n",
       "       [0.0, 1.0, 94657.16, 145077.58, 282574.31],\n",
       "       [1.0, 0.0, 91749.16, 114175.79, 294919.57],\n",
       "       [0.0, 1.0, 86419.7, 153514.11, 0.0],\n",
       "       [0.0, 0.0, 76253.86, 113867.3, 298664.47],\n",
       "       [0.0, 1.0, 78389.47, 153773.43, 299737.29],\n",
       "       [1.0, 0.0, 73994.56, 122782.75, 303319.26],\n",
       "       [1.0, 0.0, 67532.53, 105751.03, 304768.73],\n",
       "       [0.0, 1.0, 77044.01, 99281.34, 140574.81],\n",
       "       [0.0, 0.0, 64664.71, 139553.16, 137962.62],\n",
       "       [1.0, 0.0, 75328.87, 144135.98, 134050.07],\n",
       "       [0.0, 1.0, 72107.6, 127864.55, 353183.81],\n",
       "       [1.0, 0.0, 66051.52, 182645.56, 118148.2],\n",
       "       [0.0, 1.0, 65605.48, 153032.06, 107138.38],\n",
       "       [1.0, 0.0, 61994.48, 115641.28, 91131.24],\n",
       "       [0.0, 1.0, 61136.38, 152701.92, 88218.23],\n",
       "       [0.0, 0.0, 63408.86, 129219.61, 46085.25],\n",
       "       [1.0, 0.0, 55493.95, 103057.49, 214634.81],\n",
       "       [0.0, 0.0, 46426.07, 157693.92, 210797.67],\n",
       "       [0.0, 1.0, 46014.02, 85047.44, 205517.64],\n",
       "       [1.0, 0.0, 28663.76, 127056.21, 201126.82],\n",
       "       [0.0, 0.0, 44069.95, 51283.14, 197029.42],\n",
       "       [0.0, 1.0, 20229.59, 65947.93, 185265.1],\n",
       "       [0.0, 0.0, 38558.51, 82982.09, 174999.3],\n",
       "       [0.0, 0.0, 28754.33, 118546.05, 172795.67],\n",
       "       [1.0, 0.0, 27892.92, 84710.77, 164470.71],\n",
       "       [0.0, 0.0, 23640.93, 96189.63, 148001.11],\n",
       "       [0.0, 1.0, 15505.73, 127382.3, 35534.17],\n",
       "       [0.0, 0.0, 22177.74, 154806.14, 28334.72],\n",
       "       [0.0, 1.0, 1000.23, 124153.04, 1903.93],\n",
       "       [1.0, 0.0, 1315.46, 115816.21, 297114.46],\n",
       "       [0.0, 0.0, 0.0, 135426.92, 0.0],\n",
       "       [0.0, 1.0, 542.05, 51743.15, 0.0],\n",
       "       [0.0, 0.0, 0.0, 116983.8, 45173.06]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#avoiding the dummy variable trap:  \n",
    "x = x[:, 1:]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will split both variables into the test set and training set\n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 2: Fitting the Simple Linear Regression to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have fitted our regressor object to the training set so that the model can easily learn \n",
    "# the correlations between the predictor and target variables.\n",
    "regressor= LinearRegression()\n",
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Step 3: Prediction of test result</b>\n",
    "In this step, we will provide the test dataset (new observations) to the model to check whether it can predict the correct output or not.<br>\n",
    "We will create a prediction vector y_pred, and x_pred, which will contain predictions of test dataset, and prediction of training set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([103015.2 , 132582.28, 132447.74,  71976.1 , 178537.48, 116161.24,\n",
       "        67851.69,  98791.73, 113969.44, 167921.07])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "y_pred.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12908/3746713026.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train Score: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test Score: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'regressor' is not defined"
     ]
    }
   ],
   "source": [
    "print('Train Score: ', regressor.score(x_train, y_train))\n",
    "print('Test Score: ', regressor.score(x_test, y_test))\n",
    "print(\"accuracy\",regressor.accuracy(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The above score tells that our model is 95% accurate with the training dataset and 93% accurate with the test dataset.</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Elimination\n",
    "Backward elimination is a feature selection technique while building a machine learning model. It is used to remove those features that do not have a significant effect on the dependent variable or prediction of output.\n",
    "#### Steps of Backward Elimination\n",
    "Below are some main steps which are used to apply backward elimination process:<br>\n",
    "<b>Step-1:</b> Firstly, We need to select a significance level to stay in the model. (SL=0.05)<br>\n",
    "<b>Step-2:</b> Fit the complete model with all possible predictors/independent variables.<br>\n",
    "<b>Step-3:</b> Choose the predictor which has the highest P-value, such that.<br>\n",
    "<ul>\n",
    "<li>a. If P-value >SL, go to step 4.</li>\n",
    "<li>b. Else Finish, and Our model is ready.</li></ul>\n",
    "<b>Step-4:</b> Remove that predictor.<br>\n",
    "<b>Step-5:</b> Rebuild and fit the model with the remaining variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>*We will use the same model which we build in the previous chapter of MLR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step: 1- Preparation of Backward Elimination</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0.0, 1.0, 165349.2, 136897.8, 471784.1],\n",
       "       [1, 0.0, 0.0, 162597.7, 151377.59, 443898.53],\n",
       "       [1, 1.0, 0.0, 153441.51, 101145.55, 407934.54],\n",
       "       [1, 0.0, 1.0, 144372.41, 118671.85, 383199.62],\n",
       "       [1, 1.0, 0.0, 142107.34, 91391.77, 366168.42],\n",
       "       [1, 0.0, 1.0, 131876.9, 99814.71, 362861.36],\n",
       "       [1, 0.0, 0.0, 134615.46, 147198.87, 127716.82],\n",
       "       [1, 1.0, 0.0, 130298.13, 145530.06, 323876.68],\n",
       "       [1, 0.0, 1.0, 120542.52, 148718.95, 311613.29],\n",
       "       [1, 0.0, 0.0, 123334.88, 108679.17, 304981.62],\n",
       "       [1, 1.0, 0.0, 101913.08, 110594.11, 229160.95],\n",
       "       [1, 0.0, 0.0, 100671.96, 91790.61, 249744.55],\n",
       "       [1, 1.0, 0.0, 93863.75, 127320.38, 249839.44],\n",
       "       [1, 0.0, 0.0, 91992.39, 135495.07, 252664.93],\n",
       "       [1, 1.0, 0.0, 119943.24, 156547.42, 256512.92],\n",
       "       [1, 0.0, 1.0, 114523.61, 122616.84, 261776.23],\n",
       "       [1, 0.0, 0.0, 78013.11, 121597.55, 264346.06],\n",
       "       [1, 0.0, 1.0, 94657.16, 145077.58, 282574.31],\n",
       "       [1, 1.0, 0.0, 91749.16, 114175.79, 294919.57],\n",
       "       [1, 0.0, 1.0, 86419.7, 153514.11, 0.0],\n",
       "       [1, 0.0, 0.0, 76253.86, 113867.3, 298664.47],\n",
       "       [1, 0.0, 1.0, 78389.47, 153773.43, 299737.29],\n",
       "       [1, 1.0, 0.0, 73994.56, 122782.75, 303319.26],\n",
       "       [1, 1.0, 0.0, 67532.53, 105751.03, 304768.73],\n",
       "       [1, 0.0, 1.0, 77044.01, 99281.34, 140574.81],\n",
       "       [1, 0.0, 0.0, 64664.71, 139553.16, 137962.62],\n",
       "       [1, 1.0, 0.0, 75328.87, 144135.98, 134050.07],\n",
       "       [1, 0.0, 1.0, 72107.6, 127864.55, 353183.81],\n",
       "       [1, 1.0, 0.0, 66051.52, 182645.56, 118148.2],\n",
       "       [1, 0.0, 1.0, 65605.48, 153032.06, 107138.38],\n",
       "       [1, 1.0, 0.0, 61994.48, 115641.28, 91131.24],\n",
       "       [1, 0.0, 1.0, 61136.38, 152701.92, 88218.23],\n",
       "       [1, 0.0, 0.0, 63408.86, 129219.61, 46085.25],\n",
       "       [1, 1.0, 0.0, 55493.95, 103057.49, 214634.81],\n",
       "       [1, 0.0, 0.0, 46426.07, 157693.92, 210797.67],\n",
       "       [1, 0.0, 1.0, 46014.02, 85047.44, 205517.64],\n",
       "       [1, 1.0, 0.0, 28663.76, 127056.21, 201126.82],\n",
       "       [1, 0.0, 0.0, 44069.95, 51283.14, 197029.42],\n",
       "       [1, 0.0, 1.0, 20229.59, 65947.93, 185265.1],\n",
       "       [1, 0.0, 0.0, 38558.51, 82982.09, 174999.3],\n",
       "       [1, 0.0, 0.0, 28754.33, 118546.05, 172795.67],\n",
       "       [1, 1.0, 0.0, 27892.92, 84710.77, 164470.71],\n",
       "       [1, 0.0, 0.0, 23640.93, 96189.63, 148001.11],\n",
       "       [1, 0.0, 1.0, 15505.73, 127382.3, 35534.17],\n",
       "       [1, 0.0, 0.0, 22177.74, 154806.14, 28334.72],\n",
       "       [1, 0.0, 1.0, 1000.23, 124153.04, 1903.93],\n",
       "       [1, 1.0, 0.0, 1315.46, 115816.21, 297114.46],\n",
       "       [1, 0.0, 0.0, 0.0, 135426.92, 0.0],\n",
       "       [1, 0.0, 1.0, 542.05, 51743.15, 0.0],\n",
       "       [1, 0.0, 0.0, 0.0, 116983.8, 45173.06]], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as smf # it is used for the estimation of various statistical models such as OLS\n",
    "\n",
    "# adding a column in matrix of features\n",
    "x = np.append(arr = np.ones((50,1)).astype(int), values=x, axis=1)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step: 2:</b>\n",
    "<ul>\n",
    "<li>Now, we are actually going to apply a backward elimination process. Firstly we will create a new feature vector x_opt, which will only contain a set of independent features that are significantly affecting the dependent variable.</li>\n",
    "<li>Next, as per the Backward Elimination process, we need to choose a significant level(0.5), and then need to fit the model with all possible predictors. So for fitting the model, we will create a regressor_OLS object of new class OLS of statsmodels library. Then we will fit it by using the fit() method.</li>\n",
    "<li>Next we need p-value to compare with SL value, so for this we will use summary() method to get the summary table of all the values.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   169.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 05 Aug 2023</td> <th>  Prob (F-statistic):</th> <td>1.34e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:18:32</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>   1074.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.013e+04</td> <td> 6884.820</td> <td>    7.281</td> <td> 0.000</td> <td> 3.62e+04</td> <td>  6.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  198.7888</td> <td> 3371.007</td> <td>    0.059</td> <td> 0.953</td> <td>-6595.030</td> <td> 6992.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>  -41.8870</td> <td> 3256.039</td> <td>   -0.013</td> <td> 0.990</td> <td>-6604.003</td> <td> 6520.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.369</td> <td> 0.000</td> <td>    0.712</td> <td>    0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.517</td> <td> 0.608</td> <td>   -0.132</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.574</td> <td> 0.123</td> <td>   -0.008</td> <td>    0.062</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.782</td> <th>  Durbin-Watson:     </th> <td>   1.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.41e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.572</td> <th>  Cond. No.          </th> <td>1.45e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.45e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     169.9\n",
       "Date:                Sat, 05 Aug 2023   Prob (F-statistic):           1.34e-27\n",
       "Time:                        13:18:32   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1063.\n",
       "Df Residuals:                      44   BIC:                             1074.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\n",
       "x1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\n",
       "x2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\n",
       "x3             0.8060      0.046     17.369      0.000       0.712       0.900\n",
       "x4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\n",
       "x5             0.0270      0.017      1.574      0.123      -0.008       0.062\n",
       "==============================================================================\n",
       "Omnibus:                       14.782   Durbin-Watson:                   1.283\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\n",
       "Skew:                          -0.948   Prob(JB):                     2.41e-05\n",
       "Kurtosis:                       5.572   Cond. No.                     1.45e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.45e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_opt=x [:, [0,1,2,3,4,5]]\n",
    "x_opt = np.array(x_opt, dtype = float)\n",
    "regressor_OLS=smf.OLS(endog = y, exog=x_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table, we will choose the highest p-value, which is for x1=0.953 Now, we have the highest p-value which is greater than the SL value, so will remove the x1 variable (dummy variable) from the table and will refit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   217.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 05 Aug 2023</td> <th>  Prob (F-statistic):</th> <td>8.50e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:18:32</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1061.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    45</td>      <th>  BIC:               </th> <td>   1070.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.018e+04</td> <td> 6747.623</td> <td>    7.437</td> <td> 0.000</td> <td> 3.66e+04</td> <td> 6.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> -136.5042</td> <td> 2801.719</td> <td>   -0.049</td> <td> 0.961</td> <td>-5779.456</td> <td> 5506.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.8059</td> <td>    0.046</td> <td>   17.571</td> <td> 0.000</td> <td>    0.714</td> <td>    0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0269</td> <td>    0.052</td> <td>   -0.521</td> <td> 0.605</td> <td>   -0.131</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0271</td> <td>    0.017</td> <td>    1.625</td> <td> 0.111</td> <td>   -0.007</td> <td>    0.061</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.892</td> <th>  Durbin-Watson:     </th> <td>   1.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.665</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.949</td> <th>  Prob(JB):          </th> <td>1.97e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.608</td> <th>  Cond. No.          </th> <td>1.43e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.43e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.946\n",
       "Method:                 Least Squares   F-statistic:                     217.2\n",
       "Date:                Sat, 05 Aug 2023   Prob (F-statistic):           8.50e-29\n",
       "Time:                        13:18:32   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1061.\n",
       "Df Residuals:                      45   BIC:                             1070.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.018e+04   6747.623      7.437      0.000    3.66e+04    6.38e+04\n",
       "x1          -136.5042   2801.719     -0.049      0.961   -5779.456    5506.447\n",
       "x2             0.8059      0.046     17.571      0.000       0.714       0.898\n",
       "x3            -0.0269      0.052     -0.521      0.605      -0.131       0.077\n",
       "x4             0.0271      0.017      1.625      0.111      -0.007       0.061\n",
       "==============================================================================\n",
       "Omnibus:                       14.892   Durbin-Watson:                   1.284\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.665\n",
       "Skew:                          -0.949   Prob(JB):                     1.97e-05\n",
       "Kurtosis:                       5.608   Cond. No.                     1.43e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.43e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_opt=x[:, [0,2,3,4,5]]\n",
    "x_opt = np.array(x_opt, dtype = float)\n",
    "regressor_OLS=smf.OLS(endog = y, exog=x_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the output image, now five variables remain. In these variables, the highest p-value is 0.961. So we will remove it in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   296.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 05 Aug 2023</td> <th>  Prob (F-statistic):</th> <td>4.53e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:18:33</td>     <th>  Log-Likelihood:    </th> <td> -525.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th> <td>   1066.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.012e+04</td> <td> 6572.353</td> <td>    7.626</td> <td> 0.000</td> <td> 3.69e+04</td> <td> 6.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.8057</td> <td>    0.045</td> <td>   17.846</td> <td> 0.000</td> <td>    0.715</td> <td>    0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.0268</td> <td>    0.051</td> <td>   -0.526</td> <td> 0.602</td> <td>   -0.130</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0272</td> <td>    0.016</td> <td>    1.655</td> <td> 0.105</td> <td>   -0.006</td> <td>    0.060</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.838</td> <th>  Durbin-Watson:     </th> <td>   1.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.949</td> <th>  Prob(JB):          </th> <td>2.21e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.586</td> <th>  Cond. No.          </th> <td>1.40e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.948\n",
       "Method:                 Least Squares   F-statistic:                     296.0\n",
       "Date:                Sat, 05 Aug 2023   Prob (F-statistic):           4.53e-30\n",
       "Time:                        13:18:33   Log-Likelihood:                -525.39\n",
       "No. Observations:                  50   AIC:                             1059.\n",
       "Df Residuals:                      46   BIC:                             1066.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\n",
       "x1             0.8057      0.045     17.846      0.000       0.715       0.897\n",
       "x2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\n",
       "x3             0.0272      0.016      1.655      0.105      -0.006       0.060\n",
       "==============================================================================\n",
       "Omnibus:                       14.838   Durbin-Watson:                   1.282\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\n",
       "Skew:                          -0.949   Prob(JB):                     2.21e-05\n",
       "Kurtosis:                       5.586   Cond. No.                     1.40e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_opt=x[:, [0,3,4,5]]\n",
    "x_opt = np.array(x_opt, dtype = float)\n",
    "regressor_OLS=smf.OLS(endog = y, exog=x_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output image, we can see the dummy variable(x2) has been removed. And the next highest value is .602, which is still greater than .5, so we need to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   450.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 05 Aug 2023</td> <th>  Prob (F-statistic):</th> <td>2.16e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:18:33</td>     <th>  Log-Likelihood:    </th> <td> -525.54</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1057.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    47</td>      <th>  BIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.698e+04</td> <td> 2689.933</td> <td>   17.464</td> <td> 0.000</td> <td> 4.16e+04</td> <td> 5.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.7966</td> <td>    0.041</td> <td>   19.266</td> <td> 0.000</td> <td>    0.713</td> <td>    0.880</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0299</td> <td>    0.016</td> <td>    1.927</td> <td> 0.060</td> <td>   -0.001</td> <td>    0.061</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.677</td> <th>  Durbin-Watson:     </th> <td>   1.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.939</td> <th>  Prob(JB):          </th> <td>2.54e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.575</td> <th>  Cond. No.          </th> <td>5.32e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.32e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.950\n",
       "Model:                            OLS   Adj. R-squared:                  0.948\n",
       "Method:                 Least Squares   F-statistic:                     450.8\n",
       "Date:                Sat, 05 Aug 2023   Prob (F-statistic):           2.16e-31\n",
       "Time:                        13:18:33   Log-Likelihood:                -525.54\n",
       "No. Observations:                  50   AIC:                             1057.\n",
       "Df Residuals:                      47   BIC:                             1063.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\n",
       "x1             0.7966      0.041     19.266      0.000       0.713       0.880\n",
       "x2             0.0299      0.016      1.927      0.060      -0.001       0.061\n",
       "==============================================================================\n",
       "Omnibus:                       14.677   Durbin-Watson:                   1.257\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\n",
       "Skew:                          -0.939   Prob(JB):                     2.54e-05\n",
       "Kurtosis:                       5.575   Cond. No.                     5.32e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.32e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_opt=x[:, [0,3,5]]\n",
    "x_opt = np.array(x_opt, dtype = float)\n",
    "regressor_OLS=smf.OLS(endog = y, exog=x_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above output image, the variable (Admin spend) has been removed. But still, there is one variable left, which is marketing spend as it has a high p-value (0.60). So we need to remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   849.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 05 Aug 2023</td> <th>  Prob (F-statistic):</th> <td>3.50e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:18:33</td>     <th>  Log-Likelihood:    </th> <td> -527.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    48</td>      <th>  BIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.903e+04</td> <td> 2537.897</td> <td>   19.320</td> <td> 0.000</td> <td> 4.39e+04</td> <td> 5.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.8543</td> <td>    0.029</td> <td>   29.151</td> <td> 0.000</td> <td>    0.795</td> <td>    0.913</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13.727</td> <th>  Durbin-Watson:     </th> <td>   1.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  18.536</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.911</td> <th>  Prob(JB):          </th> <td>9.44e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.361</td> <th>  Cond. No.          </th> <td>1.65e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.65e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.947\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     849.8\n",
       "Date:                Sat, 05 Aug 2023   Prob (F-statistic):           3.50e-32\n",
       "Time:                        13:18:33   Log-Likelihood:                -527.44\n",
       "No. Observations:                  50   AIC:                             1059.\n",
       "Df Residuals:                      48   BIC:                             1063.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\n",
       "x1             0.8543      0.029     29.151      0.000       0.795       0.913\n",
       "==============================================================================\n",
       "Omnibus:                       13.727   Durbin-Watson:                   1.116\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\n",
       "Skew:                          -0.911   Prob(JB):                     9.44e-05\n",
       "Kurtosis:                       5.361   Cond. No.                     1.65e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.65e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_opt=x[:, [0,3]]\n",
    "x_opt = np.array(x_opt, dtype = float)\n",
    "regressor_OLS=smf.OLS(endog = y, exog=x_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>*Note: </b><i>Thus only the R&D independent variable is a significant variable for the prediction. So we can now predict efficiently using this variable.</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "<ul>\n",
    "<li>Ridge regression is one of the most robust versions of linear regression in which a small amount of bias is introduced so that we can get better long term predictions.</li>\n",
    "<li>The amount of bias added to the model is known as Ridge Regression penalty. We can compute this penalty term by multiplying with the lambda to the squared weight of each individual features.</li>\n",
    "<li>The equation for ridge regression will be:</li>\n",
    "\n",
    "$L(x,y) = Min(\\sum \\limits_{i=1}^{n}(y_{i} - w_{i}x_{i})^2 + \\lambda \\sum \\limits_{i=1}^{n}(w_{i})^2)$\n",
    "<li>A general linear or polynomial regression will fail if there is high collinearity between the independent variables, so to solve such problems, Ridge regression can be used.</li>\n",
    "<li>Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization.</li>\n",
    "<li>It helps to solve the problems if we have more parameters than samples.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step 1: Data Pre-processing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data to be trained and tested\n",
    "teams = pd.read_csv(\"teams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting teams data into train and test data\n",
    "train, test = train_test_split(teams, test_size=0.2, random_state=1)\n",
    "\n",
    "## defining predictor(independent variables) and target(dependent variable)\n",
    "predictors = [\"athletes\", \"events\"]\n",
    "target = \"medals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      "       athletes  events\n",
      "1322         6       6\n",
      "1872       119      80\n",
      "953          4       4\n",
      "1117         2       2\n",
      "1993        43      25\n",
      "...        ...     ...\n",
      "1791        40      25\n",
      "1096        36      23\n",
      "1932       719     245\n",
      "235         13      11\n",
      "1061        50      38\n",
      "\n",
      "[1611 rows x 2 columns] \n",
      "y: \n",
      "       medals\n",
      "1322       0\n",
      "1872       5\n",
      "953        0\n",
      "1117       0\n",
      "1993       0\n",
      "...      ...\n",
      "1791       1\n",
      "1096       1\n",
      "1932     264\n",
      "235        0\n",
      "1061       3\n",
      "\n",
      "[1611 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "X = train[predictors].copy()\n",
    "y = train[[target]].copy()\n",
    "print(\"X: \\n\",X,\"\\ny: \\n\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the X values using mean and standard deviation\n",
    "x_mean = X.mean()\n",
    "x_std = X.std()\n",
    "X = (X - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>athletes</th>\n",
       "      <th>events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.611000e+03</td>\n",
       "      <td>1.611000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.370681e-17</td>\n",
       "      <td>-9.923781e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.768883e-01</td>\n",
       "      <td>-7.143930e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-5.297371e-01</td>\n",
       "      <td>-6.123079e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-4.197174e-01</td>\n",
       "      <td>-4.489717e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-2.679027e-02</td>\n",
       "      <td>1.839560e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.008571e+00</td>\n",
       "      <td>4.634867e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           athletes        events\n",
       "count  1.611000e+03  1.611000e+03\n",
       "mean  -2.370681e-17 -9.923781e-18\n",
       "std    1.000000e+00  1.000000e+00\n",
       "min   -5.768883e-01 -7.143930e-01\n",
       "25%   -5.297371e-01 -6.123079e-01\n",
       "50%   -4.197174e-01 -4.489717e-01\n",
       "75%   -2.679027e-02  1.839560e-01\n",
       "max    6.008571e+00  4.634867e+00"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add an intercept term into X matrix to calculate the y-intercept coefficient\n",
    "X[\"intercept\"] = 1\n",
    "X = X[[\"intercept\"] + predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1322</th>\n",
       "      <th>1872</th>\n",
       "      <th>953</th>\n",
       "      <th>1117</th>\n",
       "      <th>1993</th>\n",
       "      <th>385</th>\n",
       "      <th>1287</th>\n",
       "      <th>1831</th>\n",
       "      <th>0</th>\n",
       "      <th>1159</th>\n",
       "      <th>...</th>\n",
       "      <th>960</th>\n",
       "      <th>847</th>\n",
       "      <th>1669</th>\n",
       "      <th>715</th>\n",
       "      <th>905</th>\n",
       "      <th>1791</th>\n",
       "      <th>1096</th>\n",
       "      <th>1932</th>\n",
       "      <th>235</th>\n",
       "      <th>1061</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>athletes</th>\n",
       "      <td>-0.537596</td>\n",
       "      <td>0.350420</td>\n",
       "      <td>-0.553313</td>\n",
       "      <td>-0.569030</td>\n",
       "      <td>-0.246829</td>\n",
       "      <td>-0.482586</td>\n",
       "      <td>-0.537596</td>\n",
       "      <td>0.138239</td>\n",
       "      <td>-0.521879</td>\n",
       "      <td>-0.152527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.199678</td>\n",
       "      <td>-0.160386</td>\n",
       "      <td>-0.529737</td>\n",
       "      <td>-0.529737</td>\n",
       "      <td>-0.341132</td>\n",
       "      <td>-0.270405</td>\n",
       "      <td>-0.301839</td>\n",
       "      <td>5.065546</td>\n",
       "      <td>-0.482586</td>\n",
       "      <td>-0.191820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>events</th>\n",
       "      <td>-0.612308</td>\n",
       "      <td>0.898552</td>\n",
       "      <td>-0.653142</td>\n",
       "      <td>-0.693976</td>\n",
       "      <td>-0.224384</td>\n",
       "      <td>-0.571474</td>\n",
       "      <td>-0.612308</td>\n",
       "      <td>0.102288</td>\n",
       "      <td>-0.571474</td>\n",
       "      <td>-0.163133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285636</td>\n",
       "      <td>-0.101882</td>\n",
       "      <td>-0.612308</td>\n",
       "      <td>-0.591891</td>\n",
       "      <td>-0.367304</td>\n",
       "      <td>-0.224384</td>\n",
       "      <td>-0.265219</td>\n",
       "      <td>4.267361</td>\n",
       "      <td>-0.510223</td>\n",
       "      <td>0.041037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1611 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1322      1872      953       1117      1993      385   \\\n",
       "intercept  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "athletes  -0.537596  0.350420 -0.553313 -0.569030 -0.246829 -0.482586   \n",
       "events    -0.612308  0.898552 -0.653142 -0.693976 -0.224384 -0.571474   \n",
       "\n",
       "               1287      1831      0         1159  ...      960       847   \\\n",
       "intercept  1.000000  1.000000  1.000000  1.000000  ...  1.000000  1.000000   \n",
       "athletes  -0.537596  0.138239 -0.521879 -0.152527  ... -0.199678 -0.160386   \n",
       "events    -0.612308  0.102288 -0.571474 -0.163133  ... -0.285636 -0.101882   \n",
       "\n",
       "               1669      715       905       1791      1096      1932  \\\n",
       "intercept  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "athletes  -0.529737 -0.529737 -0.341132 -0.270405 -0.301839  5.065546   \n",
       "events    -0.612308 -0.591891 -0.367304 -0.224384 -0.265219  4.267361   \n",
       "\n",
       "               235       1061  \n",
       "intercept  1.000000  1.000000  \n",
       "athletes  -0.482586 -0.191820  \n",
       "events    -0.510223  0.041037  \n",
       "\n",
       "[3 rows x 1611 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above data, intercept is $b_{0}$, athletes in $b_{1}$ and events is $b_{2}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step 2: Fitting your Ridge regression model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0.],\n",
       "       [0., 2., 0.],\n",
       "       [0., 0., 2.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating our penalty matrix\n",
    "alpha = 2\n",
    "I = np.identity(X.shape[1])\n",
    "penalty = alpha * I\n",
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 2., 0.],\n",
       "       [0., 0., 2.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty[0][0] = 0 # this is done because we don't want to penalize the y-intercept\n",
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>10.691496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>athletes</th>\n",
       "      <td>61.857734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>events</th>\n",
       "      <td>-34.632920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              medals\n",
       "intercept  10.691496\n",
       "athletes   61.857734\n",
       "events    -34.632920"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Equation to solve for B\n",
    "B = np.linalg.inv(X.T @ X + penalty) @ X.T @ y # '@' for matrix multiplication\n",
    "B.index = [\"intercept\", \"athletes\", \"events\"]\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_X: \n",
      "       intercept  athletes    events\n",
      "309           1 -0.553313 -0.653142\n",
      "285           1  0.594035  1.000637\n",
      "919           1 -0.144668  0.102288\n",
      "120           1  0.146098  0.531045\n",
      "585           1 -0.301839 -0.122299\n",
      "...         ...       ...       ...\n",
      "541           1 -0.380425 -0.408138\n",
      "1863          1 -0.191820  0.143122\n",
      "622           1 -0.058224  0.388126\n",
      "1070          1 -0.569030 -0.693976\n",
      "1196          1 -0.553313 -0.653142\n",
      "\n",
      "[403 rows x 3 columns] \n",
      "predictions: \n",
      "          medals\n",
      "309   -0.914959\n",
      "285   12.782156\n",
      "919   -1.799893\n",
      "120    1.337116\n",
      "585   -3.744014\n",
      "...         ...\n",
      "541    1.294285\n",
      "1863  -6.130765\n",
      "622   -6.352080\n",
      "1070  -0.472980\n",
      "1196  -0.914959\n",
      "\n",
      "[403 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# doing the same for \n",
    "test_X = test[predictors]\n",
    "test_X = (test_X - x_mean) / x_std\n",
    "test_X[\"intercept\"] = 1\n",
    "test_X = test_X[[\"intercept\"] + predictors]\n",
    "\n",
    "predictions = test_X @ B\n",
    "print(\"test_X: \\n\",test_X, \"\\npredictions: \\n\",predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_fit(train, predictors, target, alpha):\n",
    "    X = train[predictors].copy()\n",
    "    y = train[[target]].copy()\n",
    "    \n",
    "    x_mean = X.mean()\n",
    "    x_std = X.std()\n",
    "    \n",
    "    X = (X - x_mean) / x_std\n",
    "    X[\"intercept\"] = 1\n",
    "    X = X[[\"intercept\"] + predictors]\n",
    "    \n",
    "    penalty = alpha * np.identity(X.shape[1])\n",
    "    penalty[0][0] = 0\n",
    "    \n",
    "    B = np.linalg.inv(X.T @ X + penalty) @ X.T @ y\n",
    "    B.index = [\"intercept\", \"athletes\", \"events\"]\n",
    "    return B, x_mean, x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, x_mean, x_std = ridge_fit(train, predictors, target, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_predict(test, predictors, x_mean, x_std, B):\n",
    "    test_X = test[predictors]\n",
    "    test_X = (test_X - x_mean) / x_std\n",
    "    test_X[\"intercept\"] = 1\n",
    "    test_X = test_X[[\"intercept\"] + predictors]\n",
    "\n",
    "    predictions = test_X @ B\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ridge_predict(test, predictors, x_mean, x_std, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge(alpha=2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(alpha=alpha)\n",
    "ridge.fit(X[predictors], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.85773366 -34.63292036]] \n",
      "\n",
      "[10.69149597]\n"
     ]
    }
   ],
   "source": [
    "print(ridge.coef_,\"\\n\")\n",
    "print(ridge.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>8.348877e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>-3.534950e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>-2.051692e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>-3.286260e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>-1.483258e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>2.309264e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>-2.797762e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>-3.730349e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>1.048051e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>8.348877e-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            medals\n",
       "309   8.348877e-14\n",
       "285  -3.534950e-13\n",
       "919  -2.051692e-13\n",
       "120  -3.286260e-13\n",
       "585  -1.483258e-13\n",
       "...            ...\n",
       "541   2.309264e-14\n",
       "1863 -2.797762e-13\n",
       "622  -3.730349e-13\n",
       "1070  1.048051e-13\n",
       "1196  8.348877e-14\n",
       "\n",
       "[403 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_predictions = ridge.predict(test_X[predictors])\n",
    "predictions - sklearn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.309640830161113,\n",
       " 6.306044331952916,\n",
       " 6.272283376431602,\n",
       " 6.114051204717718,\n",
       " 7.156811236590466,\n",
       " 6.9780545895757315]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = []\n",
    "alphas = [10**i for i in range(-2,4)]\n",
    "\n",
    "for alpha in alphas:\n",
    "    B, x_mean, x_std = ridge_fit(train, predictors, target, alpha)\n",
    "    predictions = ridge_predict(test, predictors, x_mean, x_std, B)\n",
    "    \n",
    "    errors.append(mean_absolute_error(test[target], predictions))\n",
    "\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01, 0.1, 1, 10, 100, 1000]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
